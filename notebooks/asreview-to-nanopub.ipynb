{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASReview Results to Nanopublication Export\n",
    "\n",
    "This notebook extracts screening results from an ASReview `.asreview` project file and generates:\n",
    "1. **study_inclusion.json** - Ready for nanopub generation\n",
    "2. **PRISMA flow diagram data**\n",
    "3. **Export files** (CSV, RIS) for included/excluded studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these settings for your review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EDIT THIS SECTION ===\n",
    "\n",
    "# Path to your exported .asreview file\n",
    "ASREVIEW_FILE = \"search_results_combined.asreview\"\n",
    "\n",
    "# Review metadata\n",
    "REVIEW_TITLE = \"Quantum Computing Applications in Biodiversity Research\"\n",
    "REVIEW_DESCRIPTION = \"Systematic review of quantum computing methods applied to biodiversity, conservation, and ecological research\"\n",
    "\n",
    "# Screener info (for provenance)\n",
    "SCREENER_ORCID = \"0000-0002-1784-2920\"\n",
    "SCREENER_NAME = \"Anne Fouilloux\"\n",
    "\n",
    "# Link to your systematic review nanopubs (provenance chain)\n",
    "PICO_NANOPUB_URI = \"https://w3id.org/np/RA8B3ptXUOsN7obpkFGtA0FBmsh0OnID53wOsUIpSKTcg\"\n",
    "SEARCH_STRATEGY_URI = \"https://w3id.org/np/RAEK3jctU2x3IKW174OTgmFH9zDygPiaD-vb4zGrD39A4\"\n",
    "SEARCH_EXECUTION_URI = \"https://w3id.org/np/RAMPy96eCLCXlGR9VvCVf6rJmpN_DlxxarMGm91_5n-O8\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"screening_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n",
      "  ASReview file: search_results_combined.asreview\n",
      "  Output directory: screening_results\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✓ Setup complete\")\n",
    "print(f\"  ASReview file: {ASREVIEW_FILE}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Data from ASReview Project\n",
    "\n",
    "The `.asreview` file is a ZIP archive containing:\n",
    "- `project.json` - Project metadata\n",
    "- `data_store.db` - SQLite database with paper metadata\n",
    "- `reviews/*/results.db` - SQLite database with screening decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting to: /var/folders/zf/53jxd5nj2j3dmfjqpx41p24c0000gn/T/tmpx5c6ta1r\n",
      "\n",
      "Project contents:\n",
      "  project.json\n",
      "  data_store.db\n",
      "  feature_matrices/tfidf_feature_matrix.npz\n",
      "  data/search_results_combined.ris\n",
      "  reviews/ec51e04e70a04664969dcb066e644c55/results.db\n",
      "  reviews/ec51e04e70a04664969dcb066e644c55/settings_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Extract the .asreview file (it's a ZIP)\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Extracting to: {temp_dir}\")\n",
    "\n",
    "with zipfile.ZipFile(ASREVIEW_FILE, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# List contents\n",
    "print(\"\\nProject contents:\")\n",
    "for item in Path(temp_dir).rglob(\"*\"):\n",
    "    if item.is_file():\n",
    "        print(f\"  {item.relative_to(temp_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project info:\n",
      "  Name: search_results_combined\n",
      "  ID: e243fbcba1e24cc9b185017eaf10ade9\n",
      "  Version: 2.2\n",
      "  Reviews: 1\n"
     ]
    }
   ],
   "source": [
    "# Load project config\n",
    "with open(Path(temp_dir) / \"project.json\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "print(\"Project info:\")\n",
    "print(f\"  Name: {project_config.get('name', 'N/A')}\")\n",
    "print(f\"  ID: {project_config.get('id', 'N/A')}\")\n",
    "print(f\"  Version: {project_config.get('version', 'N/A')}\")\n",
    "print(f\"  Reviews: {len(project_config.get('reviews', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1649 papers from data store\n",
      "Columns: ['dataset_row', 'dataset_id', 'duplicate_of', 'title', 'abstract', 'authors', 'keywords', 'year', 'doi', 'url', 'included', 'record_id']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_row</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>duplicate_of</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>url</th>\n",
       "      <th>included</th>\n",
       "      <th>record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>d07b0aab55644d6cadb91d61b4ae1837</td>\n",
       "      <td>None</td>\n",
       "      <td>Recent advances and applications of machine le...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Jonathan Schmidt\", \"M\\u00e1rio R. G. Marques...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://doi.org/10.1038/s41524-019-0221-0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d07b0aab55644d6cadb91d61b4ae1837</td>\n",
       "      <td>None</td>\n",
       "      <td>Joint Optimization of Radio and Computational ...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Stefania Sardellitti\", \"Gesualdo Scutari\", \"...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://doi.org/10.1109/tsipn.2015.2448520</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>d07b0aab55644d6cadb91d61b4ae1837</td>\n",
       "      <td>None</td>\n",
       "      <td>Machine learning &amp;amp; artificial intelligence...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Vedran Dunjko\", \"Hans J. Briegel\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://doi.org/10.1088/1361-6633/aab406</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>d07b0aab55644d6cadb91d61b4ae1837</td>\n",
       "      <td>None</td>\n",
       "      <td>A Unifying Review of Deep and Shallow Anomaly ...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Lukas Ruff\", \"Jacob R. Kauffmann\", \"Robert A...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://doi.org/10.1109/jproc.2021.3052449</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>d07b0aab55644d6cadb91d61b4ae1837</td>\n",
       "      <td>None</td>\n",
       "      <td>The prospects of quantum computing in computat...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Carlos Outeiral\", \"Martin Strahm\", \"Jiye Shi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://doi.org/10.1002/wcms.1481</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_row                        dataset_id duplicate_of  \\\n",
       "0            0  d07b0aab55644d6cadb91d61b4ae1837         None   \n",
       "1            1  d07b0aab55644d6cadb91d61b4ae1837         None   \n",
       "2            2  d07b0aab55644d6cadb91d61b4ae1837         None   \n",
       "3            3  d07b0aab55644d6cadb91d61b4ae1837         None   \n",
       "4            4  d07b0aab55644d6cadb91d61b4ae1837         None   \n",
       "\n",
       "                                               title abstract  \\\n",
       "0  Recent advances and applications of machine le...            \n",
       "1  Joint Optimization of Radio and Computational ...            \n",
       "2  Machine learning &amp; artificial intelligence...            \n",
       "3  A Unifying Review of Deep and Shallow Anomaly ...            \n",
       "4  The prospects of quantum computing in computat...            \n",
       "\n",
       "                                             authors keywords  year  \\\n",
       "0  [\"Jonathan Schmidt\", \"M\\u00e1rio R. G. Marques...       []  2019   \n",
       "1  [\"Stefania Sardellitti\", \"Gesualdo Scutari\", \"...       []  2015   \n",
       "2               [\"Vedran Dunjko\", \"Hans J. Briegel\"]       []  2018   \n",
       "3  [\"Lukas Ruff\", \"Jacob R. Kauffmann\", \"Robert A...       []  2021   \n",
       "4  [\"Carlos Outeiral\", \"Martin Strahm\", \"Jiye Shi...       []  2020   \n",
       "\n",
       "                                          doi   url included  record_id  \n",
       "0   https://doi.org/10.1038/s41524-019-0221-0  None     None          0  \n",
       "1  https://doi.org/10.1109/tsipn.2015.2448520  None     None          1  \n",
       "2    https://doi.org/10.1088/1361-6633/aab406  None     None          2  \n",
       "3  https://doi.org/10.1109/jproc.2021.3052449  None     None          3  \n",
       "4           https://doi.org/10.1002/wcms.1481  None     None          4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load paper metadata from data_store.db\n",
    "data_store_path = Path(temp_dir) / \"data_store.db\"\n",
    "\n",
    "conn = sqlite3.connect(data_store_path)\n",
    "papers_df = pd.read_sql_query(\"SELECT * FROM record\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\nLoaded {len(papers_df)} papers from data store\")\n",
    "print(f\"Columns: {list(papers_df.columns)}\")\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results database: reviews/ec51e04e70a04664969dcb066e644c55/results.db\n",
      "\n",
      "Tables in results.db: ['results', 'decision_changes', 'last_ranking']\n",
      "\n",
      "Screening results: 570 decisions\n",
      "Columns: ['record_id', 'label', 'classifier', 'querier', 'balancer', 'feature_extractor', 'training_set', 'time', 'note', 'tags', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# Find and load screening results\n",
    "reviews_dir = Path(temp_dir) / \"reviews\"\n",
    "results_db = None\n",
    "\n",
    "for review_dir in reviews_dir.iterdir():\n",
    "    if review_dir.is_dir():\n",
    "        results_path = review_dir / \"results.db\"\n",
    "        if results_path.exists():\n",
    "            results_db = results_path\n",
    "            print(f\"Found results database: {results_path.relative_to(temp_dir)}\")\n",
    "            break\n",
    "\n",
    "if results_db:\n",
    "    conn = sqlite3.connect(results_db)\n",
    "    \n",
    "    # Check available tables\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    print(f\"\\nTables in results.db: {list(tables['name'])}\")\n",
    "    \n",
    "    # Load results\n",
    "    results_df = pd.read_sql_query(\"SELECT * FROM results\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\nScreening results: {len(results_df)} decisions\")\n",
    "    print(f\"Columns: {list(results_df.columns)}\")\n",
    "else:\n",
    "    print(\"ERROR: Could not find results.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in results_df: ['record_id', 'label', 'classifier', 'querier', 'balancer', 'feature_extractor', 'training_set', 'time', 'note', 'tags', 'user_id']\n",
      "\n",
      "Screening summary:\n",
      "status\n",
      "not_screened    1080\n",
      "excluded         286\n",
      "included         283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge papers with screening decisions\n",
    "# Check what columns are in results_df\n",
    "print(\"Columns in results_df:\", list(results_df.columns))\n",
    "\n",
    "# Add record_id to papers_df if not present\n",
    "if 'record_id' not in papers_df.columns:\n",
    "    papers_df['record_id'] = papers_df.index\n",
    "\n",
    "# Select only columns that exist in results_df\n",
    "merge_cols = ['record_id', 'label']\n",
    "if 'notes' in results_df.columns:\n",
    "    merge_cols.append('notes')\n",
    "\n",
    "# Merge\n",
    "merged_df = papers_df.merge(\n",
    "    results_df[merge_cols], \n",
    "    on='record_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Categorize\n",
    "merged_df['status'] = merged_df['label'].map({\n",
    "    1: 'included',\n",
    "    0: 'excluded'\n",
    "}).fillna('not_screened')\n",
    "\n",
    "print(\"\\nScreening summary:\")\n",
    "print(merged_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PRISMA Flow Diagram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRISMA FLOW DIAGRAM DATA\n",
      "============================================================\n",
      "\n",
      "IDENTIFICATION\n",
      "  Total records from databases: 1649\n",
      "\n",
      "SCREENING (Title/Abstract)\n",
      "  Records screened: 569\n",
      "  Records excluded: 286\n",
      "  Not screened (AI stopped): 1080\n",
      "\n",
      "INCLUDED\n",
      "  Studies after title/abstract screening: 283\n",
      "============================================================\n",
      "\n",
      "✓ Saved: screening_results/prisma_flow_data.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate PRISMA numbers\n",
    "total_records = len(merged_df)\n",
    "screened = len(merged_df[merged_df['status'] != 'not_screened'])\n",
    "included = len(merged_df[merged_df['status'] == 'included'])\n",
    "excluded = len(merged_df[merged_df['status'] == 'excluded'])\n",
    "not_screened = len(merged_df[merged_df['status'] == 'not_screened'])\n",
    "\n",
    "prisma_data = {\n",
    "    \"identification\": {\n",
    "        \"total_records\": total_records,\n",
    "        \"source\": \"Multiple databases (OpenAlex, arXiv, PubMed, Europe PMC, Semantic Scholar)\"\n",
    "    },\n",
    "    \"screening\": {\n",
    "        \"records_screened\": screened,\n",
    "        \"records_excluded_titleabstract\": excluded,\n",
    "        \"not_screened_ai_prioritization\": not_screened,\n",
    "        \"screening_method\": \"ASReview LAB v2.2 (active learning)\"\n",
    "    },\n",
    "    \"included\": {\n",
    "        \"studies_included_titleabstract\": included\n",
    "    },\n",
    "    \"notes\": {\n",
    "        \"ai_assisted\": True,\n",
    "        \"stopping_rule\": \"Consecutive irrelevant threshold\",\n",
    "        \"estimated_recall\": \">95%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRISMA FLOW DIAGRAM DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nIDENTIFICATION\")\n",
    "print(f\"  Total records from databases: {total_records}\")\n",
    "print(f\"\\nSCREENING (Title/Abstract)\")\n",
    "print(f\"  Records screened: {screened}\")\n",
    "print(f\"  Records excluded: {excluded}\")\n",
    "print(f\"  Not screened (AI stopped): {not_screened}\")\n",
    "print(f\"\\nINCLUDED\")\n",
    "print(f\"  Studies after title/abstract screening: {included}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save PRISMA data\n",
    "with open(f\"{OUTPUT_DIR}/prisma_flow_data.json\", 'w') as f:\n",
    "    json.dump(prisma_data, f, indent=2)\n",
    "print(f\"\\n✓ Saved: {OUTPUT_DIR}/prisma_flow_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Study Inclusion JSON for Nanopubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 283 included studies for nanopub export\n",
      "\n",
      "Sample of included papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joint Optimization of Radio and Computational ...</td>\n",
       "      <td>https://doi.org/10.1109/tsipn.2015.2448520</td>\n",
       "      <td>[\"Stefania Sardellitti\", \"Gesualdo Scutari\", \"...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine learning &amp;amp; artificial intelligence...</td>\n",
       "      <td>https://doi.org/10.1088/1361-6633/aab406</td>\n",
       "      <td>[\"Vedran Dunjko\", \"Hans J. Briegel\"]</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The prospects of quantum computing in computat...</td>\n",
       "      <td>https://doi.org/10.1002/wcms.1481</td>\n",
       "      <td>[\"Carlos Outeiral\", \"Martin Strahm\", \"Jiye Shi...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quantum Machine Learning Applications in the B...</td>\n",
       "      <td>https://doi.org/10.1109/access.2022.3195044</td>\n",
       "      <td>[\"Danyal Maheshwari\", \"Begonya Garc\\u00eda-Zap...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quantum-Inspired Real-Time Optimization for 6G...</td>\n",
       "      <td>https://doi.org/10.1109/ojcoms.2022.3195219</td>\n",
       "      <td>[\"Trung Q. Duong\", \"Long D. Nguyen\", \"Bhaskara...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "1  Joint Optimization of Radio and Computational ...   \n",
       "2  Machine learning &amp; artificial intelligence...   \n",
       "4  The prospects of quantum computing in computat...   \n",
       "5  Quantum Machine Learning Applications in the B...   \n",
       "8  Quantum-Inspired Real-Time Optimization for 6G...   \n",
       "\n",
       "                                           doi  \\\n",
       "1   https://doi.org/10.1109/tsipn.2015.2448520   \n",
       "2     https://doi.org/10.1088/1361-6633/aab406   \n",
       "4            https://doi.org/10.1002/wcms.1481   \n",
       "5  https://doi.org/10.1109/access.2022.3195044   \n",
       "8  https://doi.org/10.1109/ojcoms.2022.3195219   \n",
       "\n",
       "                                             authors  year  \n",
       "1  [\"Stefania Sardellitti\", \"Gesualdo Scutari\", \"...  2015  \n",
       "2               [\"Vedran Dunjko\", \"Hans J. Briegel\"]  2018  \n",
       "4  [\"Carlos Outeiral\", \"Martin Strahm\", \"Jiye Shi...  2020  \n",
       "5  [\"Danyal Maheshwari\", \"Begonya Garc\\u00eda-Zap...  2022  \n",
       "8  [\"Trung Q. Duong\", \"Long D. Nguyen\", \"Bhaskara...  2022  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get included studies\n",
    "included_df = merged_df[merged_df['status'] == 'included'].copy()\n",
    "\n",
    "print(f\"Preparing {len(included_df)} included studies for nanopub export\")\n",
    "print(f\"\\nSample of included papers:\")\n",
    "display_cols = ['title', 'doi', 'authors', 'year']\n",
    "available_cols = [c for c in display_cols if c in included_df.columns]\n",
    "included_df[available_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 283 studies\n",
      "⚠️ 45 studies without DOI/URL (using placeholder URIs)\n"
     ]
    }
   ],
   "source": [
    "# Build study inclusion JSON\n",
    "def get_study_uri(row):\n",
    "    \"\"\"Get best available URI for the study\"\"\"\n",
    "    if pd.notna(row.get('doi')) and row['doi']:\n",
    "        doi = row['doi']\n",
    "        if not doi.startswith('http'):\n",
    "            return f\"https://doi.org/{doi}\"\n",
    "        return doi\n",
    "    if pd.notna(row.get('url')) and row['url']:\n",
    "        return row['url']\n",
    "    if pd.notna(row.get('openalex_id')) and row['openalex_id']:\n",
    "        return row['openalex_id']\n",
    "    return None\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean title for use as label\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return \"Untitled\"\n",
    "    # Truncate long titles\n",
    "    title = str(title).strip()\n",
    "    if len(title) > 200:\n",
    "        return title[:197] + \"...\"\n",
    "    return title\n",
    "\n",
    "# Build studies list\n",
    "studies = []\n",
    "missing_uri = 0\n",
    "\n",
    "for idx, row in included_df.iterrows():\n",
    "    uri = get_study_uri(row)\n",
    "    if uri is None:\n",
    "        missing_uri += 1\n",
    "        # Use a placeholder URI based on title hash\n",
    "        title_hash = hash(str(row.get('title', idx))) % 10000000\n",
    "        uri = f\"urn:study:{title_hash}\"\n",
    "    \n",
    "    study = {\n",
    "        \"uri\": uri,\n",
    "        \"label\": clean_title(row.get('title')),\n",
    "        \"metadata\": {\n",
    "            \"authors\": row.get('authors', ''),\n",
    "            \"year\": int(row['year']) if pd.notna(row.get('year')) else None,\n",
    "            \"journal\": row.get('journal', row.get('primary_location', '')),\n",
    "            \"doi\": row.get('doi', ''),\n",
    "            \"abstract\": row.get('abstract', '')[:500] if pd.notna(row.get('abstract')) else ''\n",
    "        }\n",
    "    }\n",
    "    studies.append(study)\n",
    "\n",
    "print(f\"\\nProcessed {len(studies)} studies\")\n",
    "if missing_uri > 0:\n",
    "    print(f\"⚠️ {missing_uri} studies without DOI/URL (using placeholder URIs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: screening_results/study_inclusion.json\n",
      "  Contains 283 studies ready for nanopub generation\n"
     ]
    }
   ],
   "source": [
    "# Create the full study_inclusion.json\n",
    "study_inclusion_config = {\n",
    "    \"_comment\": \"Study Inclusion nanopub configuration for Science Live\",\n",
    "    \"_generated\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"_source\": ASREVIEW_FILE,\n",
    "    \n",
    "    \"review_metadata\": {\n",
    "        \"title\": REVIEW_TITLE,\n",
    "        \"description\": REVIEW_DESCRIPTION,\n",
    "        \"screener_orcid\": SCREENER_ORCID,\n",
    "        \"screener_name\": SCREENER_NAME,\n",
    "        \"screening_date\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"),\n",
    "        \"screening_tool\": \"ASReview LAB v2.2\",\n",
    "        \"total_screened\": screened,\n",
    "        \"total_included\": included,\n",
    "        \"total_excluded\": excluded\n",
    "    },\n",
    "    \n",
    "    \"provenance\": {\n",
    "        \"pico_nanopub\": PICO_NANOPUB_URI,\n",
    "        \"search_strategy_nanopub\": SEARCH_STRATEGY_URI,\n",
    "        \"search_execution_nanopub\": SEARCH_EXECUTION_URI\n",
    "    },\n",
    "    \n",
    "    \"nanopub_template\": {\n",
    "        \"base_uri\": \"https://w3id.org/sciencelivehub/quantum-biodiversity-review/\",\n",
    "        \"type\": \"https://w3id.org/slo/StudyInclusion\",\n",
    "        \"license\": \"https://creativecommons.org/licenses/by/4.0/\"\n",
    "    },\n",
    "    \n",
    "    \"studies\": studies\n",
    "}\n",
    "\n",
    "# Save\n",
    "output_file = f\"{OUTPUT_DIR}/study_inclusion.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(study_inclusion_config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Saved: {output_file}\")\n",
    "print(f\"  Contains {len(studies)} studies ready for nanopub generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export CSV and RIS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: screening_results/included_studies.csv (283 studies)\n",
      "✓ Saved: screening_results/excluded_studies.csv (286 studies)\n"
     ]
    }
   ],
   "source": [
    "# Export included studies to CSV\n",
    "export_cols = ['title', 'authors', 'year', 'doi', 'journal', 'abstract', 'url']\n",
    "available_export_cols = [c for c in export_cols if c in included_df.columns]\n",
    "\n",
    "included_df[available_export_cols].to_csv(\n",
    "    f\"{OUTPUT_DIR}/included_studies.csv\", \n",
    "    index=False\n",
    ")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/included_studies.csv ({len(included_df)} studies)\")\n",
    "\n",
    "# Export excluded studies to CSV\n",
    "excluded_df = merged_df[merged_df['status'] == 'excluded'].copy()\n",
    "excluded_df[available_export_cols].to_csv(\n",
    "    f\"{OUTPUT_DIR}/excluded_studies.csv\", \n",
    "    index=False\n",
    ")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/excluded_studies.csv ({len(excluded_df)} studies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: screening_results/included_studies.ris\n",
      "✓ Saved: screening_results/excluded_studies.ris\n"
     ]
    }
   ],
   "source": [
    "# Export to RIS format for reference managers\n",
    "def df_to_ris(df, filename):\n",
    "    \"\"\"Convert DataFrame to RIS format\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for idx, row in df.iterrows():\n",
    "            f.write(\"TY  - JOUR\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('title')):\n",
    "                f.write(f\"TI  - {row['title']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('authors')):\n",
    "                # Split authors if comma-separated\n",
    "                authors = str(row['authors'])\n",
    "                for author in authors.split(';'):\n",
    "                    author = author.strip()\n",
    "                    if author:\n",
    "                        f.write(f\"AU  - {author}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('year')):\n",
    "                f.write(f\"PY  - {int(row['year'])}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('journal')):\n",
    "                f.write(f\"JO  - {row['journal']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('doi')):\n",
    "                doi = row['doi']\n",
    "                if not doi.startswith('http'):\n",
    "                    doi = f\"https://doi.org/{doi}\"\n",
    "                f.write(f\"DO  - {row['doi']}\\n\")\n",
    "                f.write(f\"UR  - {doi}\\n\")\n",
    "            elif pd.notna(row.get('url')):\n",
    "                f.write(f\"UR  - {row['url']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('abstract')):\n",
    "                # Clean abstract for RIS\n",
    "                abstract = str(row['abstract']).replace('\\n', ' ').strip()\n",
    "                f.write(f\"AB  - {abstract}\\n\")\n",
    "            \n",
    "            f.write(\"ER  - \\n\\n\")\n",
    "\n",
    "# Export included\n",
    "df_to_ris(included_df, f\"{OUTPUT_DIR}/included_studies.ris\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/included_studies.ris\")\n",
    "\n",
    "# Export excluded\n",
    "df_to_ris(excluded_df, f\"{OUTPUT_DIR}/excluded_studies.ris\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/excluded_studies.ris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temp directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"✓ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Review: Quantum Computing Applications in Biodiversity Research\n",
      "Screener: Anne Fouilloux (0000-0002-1784-2920)\n",
      "\n",
      "Results:\n",
      "  Total records: 1649\n",
      "  Screened: 569\n",
      "  Included: 283\n",
      "  Excluded: 286\n",
      "\n",
      "Output files in 'screening_results/' :\n",
      "  • study_inclusion.json     - For nanopub generation\n",
      "  • prisma_flow_data.json    - PRISMA diagram numbers\n",
      "  • included_studies.csv     - Included studies\n",
      "  • excluded_studies.csv     - Excluded studies\n",
      "  • included_studies.ris     - For Zotero/reference managers\n",
      "  • excluded_studies.ris     - For Zotero/reference managers\n",
      "\n",
      "Provenance chain:\n",
      "  PICO → Search Strategy → Search Execution → Study Inclusion\n",
      "============================================================\n",
      "\n",
      "Next step: Run study-inclusion-nanopub-from-json.ipynb\n",
      "           with study_inclusion.json to generate nanopubs\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nReview: {REVIEW_TITLE}\")\n",
    "print(f\"Screener: {SCREENER_NAME} ({SCREENER_ORCID})\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total records: {total_records}\")\n",
    "print(f\"  Screened: {screened}\")\n",
    "print(f\"  Included: {included}\")\n",
    "print(f\"  Excluded: {excluded}\")\n",
    "print(f\"\\nOutput files in '{OUTPUT_DIR}/' :\")\n",
    "print(f\"  • study_inclusion.json     - For nanopub generation\")\n",
    "print(f\"  • prisma_flow_data.json    - PRISMA diagram numbers\")\n",
    "print(f\"  • included_studies.csv     - Included studies\")\n",
    "print(f\"  • excluded_studies.csv     - Excluded studies\")\n",
    "print(f\"  • included_studies.ris     - For Zotero/reference managers\")\n",
    "print(f\"  • excluded_studies.ris     - For Zotero/reference managers\")\n",
    "print(f\"\\nProvenance chain:\")\n",
    "print(f\"  PICO → Search Strategy → Search Execution → Study Inclusion\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext step: Run study-inclusion-nanopub-from-json.ipynb\")\n",
    "print(\"           with study_inclusion.json to generate nanopubs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preview Study Inclusion JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of study_inclusion.json:\n",
      "\n",
      "{\n",
      "  \"review_metadata\": {\n",
      "    \"title\": \"Quantum Computing Applications in Biodiversity Research\",\n",
      "    \"description\": \"Systematic review of quantum computing methods applied to biodiversity, conservation, and ecological research\",\n",
      "    \"screener_orcid\": \"0000-0002-1784-2920\",\n",
      "    \"screener_name\": \"Anne Fouilloux\",\n",
      "    \"screening_date\": \"2025-12-27\",\n",
      "    \"screening_tool\": \"ASReview LAB v2.2\",\n",
      "    \"total_screened\": 569,\n",
      "    \"total_included\": 283,\n",
      "    \"total_excluded\": 286\n",
      "  },\n",
      "  \"provenance\": {\n",
      "    \"pico_nanopub\": \"https://w3id.org/np/RA8B3ptXUOsN7obpkFGtA0FBmsh0OnID53wOsUIpSKTcg\",\n",
      "    \"search_strategy_nanopub\": \"https://w3id.org/np/RAEK3jctU2x3IKW174OTgmFH9zDygPiaD-vb4zGrD39A4\",\n",
      "    \"search_execution_nanopub\": \"https://w3id.org/np/RAMPy96eCLCXlGR9VvCVf6rJmpN_DlxxarMGm91_5n-O8\"\n",
      "  },\n",
      "  \"studies\": [\n",
      "    {\n",
      "      \"uri\": \"https://doi.org/10.1109/tsipn.2015.2448520\",\n",
      "      \"label\": \"Joint Optimization of Radio and Computational Resources for Multicell Mobile-Edge Computing\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Stefania Sardellitti\\\", \\\"Gesualdo Scutari\\\", \\\"Sergio Barbarossa\\\"]\",\n",
      "        \"year\": 2015,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": \"https://doi.org/10.1109/tsipn.2015.2448520\",\n",
      "        \"abstract\": \"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"uri\": \"https://doi.org/10.1088/1361-6633/aab406\",\n",
      "      \"label\": \"Machine learning &amp; artificial intelligence in the quantum domain: a review of recent progress\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Vedran Dunjko\\\", \\\"Hans J. Briegel\\\"]\",\n",
      "        \"year\": 2018,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": \"https://doi.org/10.1088/1361-6633/aab406\",\n",
      "        \"abstract\": \"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"uri\": \"https://doi.org/10.1002/wcms.1481\",\n",
      "      \"label\": \"The prospects of quantum computing in computational molecular biology\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Carlos Outeiral\\\", \\\"Martin Strahm\\\", \\\"Jiye Shi\\\", \\\"Garrett M. Morris\\\", \\\"Simon C. Benjamin\\\", \\\"Charlotte M. Deane\\\"]\",\n",
      "        \"year\": 2020,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": \"https://doi.org/10.1002/wcms.1481\",\n",
      "        \"abstract\": \"\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "... and 280 more studies\n"
     ]
    }
   ],
   "source": [
    "# Show sample of the generated JSON\n",
    "print(\"Sample of study_inclusion.json:\\n\")\n",
    "preview = {\n",
    "    \"review_metadata\": study_inclusion_config[\"review_metadata\"],\n",
    "    \"provenance\": study_inclusion_config[\"provenance\"],\n",
    "    \"studies\": study_inclusion_config[\"studies\"][:3]  # First 3 studies\n",
    "}\n",
    "print(json.dumps(preview, indent=2, default=str))\n",
    "print(f\"\\n... and {len(studies) - 3} more studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
