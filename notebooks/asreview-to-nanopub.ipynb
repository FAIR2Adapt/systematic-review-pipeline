{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASReview Results to Nanopublication Export\n",
    "\n",
    "This notebook extracts screening results from an ASReview `.asreview` project file and generates:\n",
    "1. **study_inclusion.json** - Ready for nanopub generation\n",
    "2. **PRISMA flow diagram data**\n",
    "3. **Export files** (CSV, RIS) for included/excluded studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these settings for your review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EDIT THIS SECTION ===\n",
    "\n",
    "# Path to your exported .asreview file\n",
    "ASREVIEW_FILE = \"search_results_combined.asreview\"\n",
    "\n",
    "# Review metadata\n",
    "REVIEW_TITLE = \"Quantum Computing Applications in Biodiversity Research\"\n",
    "REVIEW_DESCRIPTION = \"Systematic review of quantum computing methods applied to biodiversity, conservation, and ecological research\"\n",
    "\n",
    "# Screener info (for provenance)\n",
    "SCREENER_ORCID = \"0000-0002-1784-2920\"\n",
    "SCREENER_NAME = \"Anne Fouilloux\"\n",
    "\n",
    "# Link to your systematic review nanopubs (provenance chain)\n",
    "PICO_NANOPUB_URI = \"https://w3id.org/np/RA8B3ptXUOsN7obpkFGtA0FBmsh0OnID53wOsUIpSKTcg\"\n",
    "SEARCH_STRATEGY_URI = \"https://w3id.org/np/RAEK3jctU2x3IKW174OTgmFH9zDygPiaD-vb4zGrD39A4\"\n",
    "SEARCH_EXECUTION_URI = \"https://w3id.org/np/RAMPy96eCLCXlGR9VvCVf6rJmpN_DlxxarMGm91_5n-O8\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"screening_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your exported .asreview file\n",
    "ASREVIEW_FILE = \"wildfire-sentinel2-ml/results/wildfire-sentinel2-ml-no-duplicates.asreview\"\n",
    "\n",
    "# Review metadata\n",
    "REVIEW_TITLE = \"Machine Learning Algorithms for Wildfire Detection and Burned Area Mapping using Sentinel-2\"\n",
    "REVIEW_DESCRIPTION = \"Systematic review on Machine Learning Algorithms for Wildfire Detection and Burned Area Mapping using Sentinel-2\"\n",
    "\n",
    "# Screener info (for provenance)\n",
    "SCREENER_ORCID = \"0000-0002-1784-2920\"\n",
    "SCREENER_NAME = \"Anne Fouilloux\"\n",
    "\n",
    "# Link to your systematic review nanopubs (provenance chain)\n",
    "PICO_NANOPUB_URI = \"https://w3id.org/np/RAjO8tdVOla9I77PeXF4iY92ULngrpx5_ZSKFkVrCmsW0\"\n",
    "SEARCH_STRATEGY_URI = \" \"\n",
    "SEARCH_EXECUTION_URI = \" \"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"wildfire-sentinel2-ml/screening_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n",
      "  ASReview file: wildfire-sentinel2-ml/results/wildfire-sentinel2-ml-no-duplicates.asreview\n",
      "  Output directory: wildfire-sentinel2-ml/screening_results\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✓ Setup complete\")\n",
    "print(f\"  ASReview file: {ASREVIEW_FILE}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Data from ASReview Project\n",
    "\n",
    "The `.asreview` file is a ZIP archive containing:\n",
    "- `project.json` - Project metadata\n",
    "- `data_store.db` - SQLite database with paper metadata\n",
    "- `reviews/*/results.db` - SQLite database with screening decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting to: /var/folders/zf/53jxd5nj2j3dmfjqpx41p24c0000gn/T/tmpieghkk0m\n",
      "\n",
      "Project contents:\n",
      "  project.json\n",
      "  data_store.db\n",
      "  feature_matrices/tfidf_feature_matrix.npz\n",
      "  data/wildfire-sentinel2-ml-no-duplicates.ris\n",
      "  reviews/dd8a86cb9f02451c9a9dde42ae792afe/results.db\n",
      "  reviews/dd8a86cb9f02451c9a9dde42ae792afe/settings_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Extract the .asreview file (it's a ZIP)\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Extracting to: {temp_dir}\")\n",
    "\n",
    "with zipfile.ZipFile(ASREVIEW_FILE, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# List contents\n",
    "print(\"\\nProject contents:\")\n",
    "for item in Path(temp_dir).rglob(\"*\"):\n",
    "    if item.is_file():\n",
    "        print(f\"  {item.relative_to(temp_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project info:\n",
      "  Name: wildfire-sentinel2-ml-no-duplicates\n",
      "  ID: b50068f3a2e2439fa80b64922d8418ff\n",
      "  Version: 2.2\n",
      "  Reviews: 1\n"
     ]
    }
   ],
   "source": [
    "# Load project config\n",
    "with open(Path(temp_dir) / \"project.json\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "print(\"Project info:\")\n",
    "print(f\"  Name: {project_config.get('name', 'N/A')}\")\n",
    "print(f\"  ID: {project_config.get('id', 'N/A')}\")\n",
    "print(f\"  Version: {project_config.get('version', 'N/A')}\")\n",
    "print(f\"  Reviews: {len(project_config.get('reviews', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 450 papers from data store\n",
      "Columns: ['dataset_row', 'dataset_id', 'duplicate_of', 'title', 'abstract', 'authors', 'keywords', 'year', 'doi', 'url', 'included', 'record_id']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_row</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>duplicate_of</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>url</th>\n",
       "      <th>included</th>\n",
       "      <th>record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2806332e78004d39a273e4af26696f59</td>\n",
       "      <td>None</td>\n",
       "      <td>FireScope: Wildfire Risk Prediction with a Cha...</td>\n",
       "      <td>Predicting wildfire risk is a reasoning-intens...</td>\n",
       "      <td>[\"Mario Markov\", \"Stefan Maria Ailuro\", \"Luc V...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2806332e78004d39a273e4af26696f59</td>\n",
       "      <td>None</td>\n",
       "      <td>Assessment of the January 2025 Los Angeles Cou...</td>\n",
       "      <td>This study presents a comprehensive analysis o...</td>\n",
       "      <td>[\"Seyd Teymoor Seydi\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2806332e78004d39a273e4af26696f59</td>\n",
       "      <td>None</td>\n",
       "      <td>On the Generalizability of Foundation Models f...</td>\n",
       "      <td>Foundation models pre-trained using self-super...</td>\n",
       "      <td>[\"Yi-Chia Chang\", \"Adam J. Stewart\", \"Favyen B...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2806332e78004d39a273e4af26696f59</td>\n",
       "      <td>None</td>\n",
       "      <td>Sen2Fire: A Challenging Benchmark Dataset for ...</td>\n",
       "      <td>Utilizing satellite imagery for wildfire detec...</td>\n",
       "      <td>[\"Yonghao Xu\", \"Amanda Berg\", \"Leif Haglund\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024</td>\n",
       "      <td>10.1109/IGARSS53475.2024.10641441</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2806332e78004d39a273e4af26696f59</td>\n",
       "      <td>None</td>\n",
       "      <td>CaBuAr: California Burned Areas dataset for de...</td>\n",
       "      <td>Forest wildfires represent one of the catastro...</td>\n",
       "      <td>[\"Daniele Rege Cambrin\", \"Luca Colomba\", \"Paol...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024</td>\n",
       "      <td>10.1109/MGRS.2023.3292467</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_row                        dataset_id duplicate_of  \\\n",
       "0            0  2806332e78004d39a273e4af26696f59         None   \n",
       "1            1  2806332e78004d39a273e4af26696f59         None   \n",
       "2            2  2806332e78004d39a273e4af26696f59         None   \n",
       "3            3  2806332e78004d39a273e4af26696f59         None   \n",
       "4            4  2806332e78004d39a273e4af26696f59         None   \n",
       "\n",
       "                                               title  \\\n",
       "0  FireScope: Wildfire Risk Prediction with a Cha...   \n",
       "1  Assessment of the January 2025 Los Angeles Cou...   \n",
       "2  On the Generalizability of Foundation Models f...   \n",
       "3  Sen2Fire: A Challenging Benchmark Dataset for ...   \n",
       "4  CaBuAr: California Burned Areas dataset for de...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Predicting wildfire risk is a reasoning-intens...   \n",
       "1  This study presents a comprehensive analysis o...   \n",
       "2  Foundation models pre-trained using self-super...   \n",
       "3  Utilizing satellite imagery for wildfire detec...   \n",
       "4  Forest wildfires represent one of the catastro...   \n",
       "\n",
       "                                             authors keywords  year  \\\n",
       "0  [\"Mario Markov\", \"Stefan Maria Ailuro\", \"Luc V...       []  2025   \n",
       "1                             [\"Seyd Teymoor Seydi\"]       []  2025   \n",
       "2  [\"Yi-Chia Chang\", \"Adam J. Stewart\", \"Favyen B...       []  2024   \n",
       "3      [\"Yonghao Xu\", \"Amanda Berg\", \"Leif Haglund\"]       []  2024   \n",
       "4  [\"Daniele Rege Cambrin\", \"Luca Colomba\", \"Paol...       []  2024   \n",
       "\n",
       "                                 doi   url included  record_id  \n",
       "0                               None  None     None          0  \n",
       "1                               None  None     None          1  \n",
       "2                               None  None     None          2  \n",
       "3  10.1109/IGARSS53475.2024.10641441  None     None          3  \n",
       "4          10.1109/MGRS.2023.3292467  None     None          4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load paper metadata from data_store.db\n",
    "data_store_path = Path(temp_dir) / \"data_store.db\"\n",
    "\n",
    "conn = sqlite3.connect(data_store_path)\n",
    "papers_df = pd.read_sql_query(\"SELECT * FROM record\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\nLoaded {len(papers_df)} papers from data store\")\n",
    "print(f\"Columns: {list(papers_df.columns)}\")\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results database: reviews/dd8a86cb9f02451c9a9dde42ae792afe/results.db\n",
      "\n",
      "Tables in results.db: ['results', 'decision_changes', 'last_ranking']\n",
      "\n",
      "Screening results: 261 decisions\n",
      "Columns: ['record_id', 'label', 'classifier', 'querier', 'balancer', 'feature_extractor', 'training_set', 'time', 'note', 'tags', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# Find and load screening results\n",
    "reviews_dir = Path(temp_dir) / \"reviews\"\n",
    "results_db = None\n",
    "\n",
    "for review_dir in reviews_dir.iterdir():\n",
    "    if review_dir.is_dir():\n",
    "        results_path = review_dir / \"results.db\"\n",
    "        if results_path.exists():\n",
    "            results_db = results_path\n",
    "            print(f\"Found results database: {results_path.relative_to(temp_dir)}\")\n",
    "            break\n",
    "\n",
    "if results_db:\n",
    "    conn = sqlite3.connect(results_db)\n",
    "    \n",
    "    # Check available tables\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    print(f\"\\nTables in results.db: {list(tables['name'])}\")\n",
    "    \n",
    "    # Load results\n",
    "    results_df = pd.read_sql_query(\"SELECT * FROM results\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\nScreening results: {len(results_df)} decisions\")\n",
    "    print(f\"Columns: {list(results_df.columns)}\")\n",
    "else:\n",
    "    print(\"ERROR: Could not find results.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in results_df: ['record_id', 'label', 'classifier', 'querier', 'balancer', 'feature_extractor', 'training_set', 'time', 'note', 'tags', 'user_id']\n",
      "\n",
      "Screening summary:\n",
      "status\n",
      "not_screened    190\n",
      "included        150\n",
      "excluded        110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge papers with screening decisions\n",
    "# Check what columns are in results_df\n",
    "print(\"Columns in results_df:\", list(results_df.columns))\n",
    "\n",
    "# Add record_id to papers_df if not present\n",
    "if 'record_id' not in papers_df.columns:\n",
    "    papers_df['record_id'] = papers_df.index\n",
    "\n",
    "# Select only columns that exist in results_df\n",
    "merge_cols = ['record_id', 'label']\n",
    "if 'notes' in results_df.columns:\n",
    "    merge_cols.append('notes')\n",
    "\n",
    "# Merge\n",
    "merged_df = papers_df.merge(\n",
    "    results_df[merge_cols], \n",
    "    on='record_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Categorize\n",
    "merged_df['status'] = merged_df['label'].map({\n",
    "    1: 'included',\n",
    "    0: 'excluded'\n",
    "}).fillna('not_screened')\n",
    "\n",
    "print(\"\\nScreening summary:\")\n",
    "print(merged_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PRISMA Flow Diagram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRISMA FLOW DIAGRAM DATA\n",
      "============================================================\n",
      "\n",
      "IDENTIFICATION\n",
      "  Total records from databases: 450\n",
      "\n",
      "SCREENING (Title/Abstract)\n",
      "  Records screened: 260\n",
      "  Records excluded: 110\n",
      "  Not screened (AI stopped): 190\n",
      "\n",
      "INCLUDED\n",
      "  Studies after title/abstract screening: 150\n",
      "============================================================\n",
      "\n",
      "✓ Saved: wildfire-sentinel2-ml/screening_results/prisma_flow_data.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate PRISMA numbers\n",
    "total_records = len(merged_df)\n",
    "screened = len(merged_df[merged_df['status'] != 'not_screened'])\n",
    "included = len(merged_df[merged_df['status'] == 'included'])\n",
    "excluded = len(merged_df[merged_df['status'] == 'excluded'])\n",
    "not_screened = len(merged_df[merged_df['status'] == 'not_screened'])\n",
    "\n",
    "prisma_data = {\n",
    "    \"identification\": {\n",
    "        \"total_records\": total_records,\n",
    "        \"source\": \"Multiple databases (OpenAlex, arXiv, PubMed, Europe PMC, Semantic Scholar)\"\n",
    "    },\n",
    "    \"screening\": {\n",
    "        \"records_screened\": screened,\n",
    "        \"records_excluded_titleabstract\": excluded,\n",
    "        \"not_screened_ai_prioritization\": not_screened,\n",
    "        \"screening_method\": \"ASReview LAB v2.2 (active learning)\"\n",
    "    },\n",
    "    \"included\": {\n",
    "        \"studies_included_titleabstract\": included\n",
    "    },\n",
    "    \"notes\": {\n",
    "        \"ai_assisted\": True,\n",
    "        \"stopping_rule\": \"Consecutive irrelevant threshold\",\n",
    "        \"estimated_recall\": \">95%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRISMA FLOW DIAGRAM DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nIDENTIFICATION\")\n",
    "print(f\"  Total records from databases: {total_records}\")\n",
    "print(f\"\\nSCREENING (Title/Abstract)\")\n",
    "print(f\"  Records screened: {screened}\")\n",
    "print(f\"  Records excluded: {excluded}\")\n",
    "print(f\"  Not screened (AI stopped): {not_screened}\")\n",
    "print(f\"\\nINCLUDED\")\n",
    "print(f\"  Studies after title/abstract screening: {included}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save PRISMA data\n",
    "with open(f\"{OUTPUT_DIR}/prisma_flow_data.json\", 'w') as f:\n",
    "    json.dump(prisma_data, f, indent=2)\n",
    "print(f\"\\n✓ Saved: {OUTPUT_DIR}/prisma_flow_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Study Inclusion JSON for Nanopubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 150 included studies for nanopub export\n",
      "\n",
      "Sample of included papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FireScope: Wildfire Risk Prediction with a Cha...</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"Mario Markov\", \"Stefan Maria Ailuro\", \"Luc V...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assessment of the January 2025 Los Angeles Cou...</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"Seyd Teymoor Seydi\"]</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sen2Fire: A Challenging Benchmark Dataset for ...</td>\n",
       "      <td>10.1109/IGARSS53475.2024.10641441</td>\n",
       "      <td>[\"Yonghao Xu\", \"Amanda Berg\", \"Leif Haglund\"]</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CaBuAr: California Burned Areas dataset for de...</td>\n",
       "      <td>10.1109/MGRS.2023.3292467</td>\n",
       "      <td>[\"Daniele Rege Cambrin\", \"Luca Colomba\", \"Paol...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Burned Area Detection with Sentinel-2A Data: U...</td>\n",
       "      <td>10.5194/isprs-annals-x-5-2024-251-2024</td>\n",
       "      <td>[\"Elif Ozlem Yilmaz\", \"T. Kavzoglu\"]</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  FireScope: Wildfire Risk Prediction with a Cha...   \n",
       "1  Assessment of the January 2025 Los Angeles Cou...   \n",
       "3  Sen2Fire: A Challenging Benchmark Dataset for ...   \n",
       "4  CaBuAr: California Burned Areas dataset for de...   \n",
       "7  Burned Area Detection with Sentinel-2A Data: U...   \n",
       "\n",
       "                                      doi  \\\n",
       "0                                    None   \n",
       "1                                    None   \n",
       "3       10.1109/IGARSS53475.2024.10641441   \n",
       "4               10.1109/MGRS.2023.3292467   \n",
       "7  10.5194/isprs-annals-x-5-2024-251-2024   \n",
       "\n",
       "                                             authors  year  \n",
       "0  [\"Mario Markov\", \"Stefan Maria Ailuro\", \"Luc V...  2025  \n",
       "1                             [\"Seyd Teymoor Seydi\"]  2025  \n",
       "3      [\"Yonghao Xu\", \"Amanda Berg\", \"Leif Haglund\"]  2024  \n",
       "4  [\"Daniele Rege Cambrin\", \"Luca Colomba\", \"Paol...  2024  \n",
       "7               [\"Elif Ozlem Yilmaz\", \"T. Kavzoglu\"]  2024  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get included studies\n",
    "included_df = merged_df[merged_df['status'] == 'included'].copy()\n",
    "\n",
    "print(f\"Preparing {len(included_df)} included studies for nanopub export\")\n",
    "print(f\"\\nSample of included papers:\")\n",
    "display_cols = ['title', 'doi', 'authors', 'year']\n",
    "available_cols = [c for c in display_cols if c in included_df.columns]\n",
    "included_df[available_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 150 studies\n",
      "⚠️ 12 studies without DOI/URL (using placeholder URIs)\n"
     ]
    }
   ],
   "source": [
    "# Build study inclusion JSON\n",
    "def get_study_uri(row):\n",
    "    \"\"\"Get best available URI for the study\"\"\"\n",
    "    if pd.notna(row.get('doi')) and row['doi']:\n",
    "        doi = row['doi']\n",
    "        if not doi.startswith('http'):\n",
    "            return f\"https://doi.org/{doi}\"\n",
    "        return doi\n",
    "    if pd.notna(row.get('url')) and row['url']:\n",
    "        return row['url']\n",
    "    if pd.notna(row.get('openalex_id')) and row['openalex_id']:\n",
    "        return row['openalex_id']\n",
    "    return None\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean title for use as label\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return \"Untitled\"\n",
    "    # Truncate long titles\n",
    "    title = str(title).strip()\n",
    "    if len(title) > 200:\n",
    "        return title[:197] + \"...\"\n",
    "    return title\n",
    "\n",
    "# Build studies list\n",
    "studies = []\n",
    "missing_uri = 0\n",
    "\n",
    "for idx, row in included_df.iterrows():\n",
    "    uri = get_study_uri(row)\n",
    "    if uri is None:\n",
    "        missing_uri += 1\n",
    "        # Use a placeholder URI based on title hash\n",
    "        title_hash = hash(str(row.get('title', idx))) % 10000000\n",
    "        uri = f\"urn:study:{title_hash}\"\n",
    "    \n",
    "    study = {\n",
    "        \"uri\": uri,\n",
    "        \"label\": clean_title(row.get('title')),\n",
    "        \"metadata\": {\n",
    "            \"authors\": row.get('authors', ''),\n",
    "            \"year\": int(row['year']) if pd.notna(row.get('year')) else None,\n",
    "            \"journal\": row.get('journal', row.get('primary_location', '')),\n",
    "            \"doi\": row.get('doi', ''),\n",
    "            \"abstract\": row.get('abstract', '')[:500] if pd.notna(row.get('abstract')) else ''\n",
    "        }\n",
    "    }\n",
    "    studies.append(study)\n",
    "\n",
    "print(f\"\\nProcessed {len(studies)} studies\")\n",
    "if missing_uri > 0:\n",
    "    print(f\"⚠️ {missing_uri} studies without DOI/URL (using placeholder URIs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: wildfire-sentinel2-ml/screening_results/study_inclusion.json\n",
      "  Contains 150 studies ready for nanopub generation\n"
     ]
    }
   ],
   "source": [
    "# Create the full study_inclusion.json\n",
    "study_inclusion_config = {\n",
    "    \"_comment\": \"Study Inclusion nanopub configuration for Science Live\",\n",
    "    \"_generated\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"_source\": ASREVIEW_FILE,\n",
    "    \n",
    "    \"review_metadata\": {\n",
    "        \"title\": REVIEW_TITLE,\n",
    "        \"description\": REVIEW_DESCRIPTION,\n",
    "        \"screener_orcid\": SCREENER_ORCID,\n",
    "        \"screener_name\": SCREENER_NAME,\n",
    "        \"screening_date\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"),\n",
    "        \"screening_tool\": \"ASReview LAB v2.2\",\n",
    "        \"total_screened\": screened,\n",
    "        \"total_included\": included,\n",
    "        \"total_excluded\": excluded\n",
    "    },\n",
    "    \n",
    "    \"provenance\": {\n",
    "        \"pico_nanopub\": PICO_NANOPUB_URI,\n",
    "        \"search_strategy_nanopub\": SEARCH_STRATEGY_URI,\n",
    "        \"search_execution_nanopub\": SEARCH_EXECUTION_URI\n",
    "    },\n",
    "    \n",
    "    \"nanopub_template\": {\n",
    "        \"base_uri\": \"https://w3id.org/sciencelivehub/quantum-biodiversity-review/\",\n",
    "        \"type\": \"https://w3id.org/slo/StudyInclusion\",\n",
    "        \"license\": \"https://creativecommons.org/licenses/by/4.0/\"\n",
    "    },\n",
    "    \n",
    "    \"studies\": studies\n",
    "}\n",
    "\n",
    "# Save\n",
    "output_file = f\"{OUTPUT_DIR}/study_inclusion.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(study_inclusion_config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Saved: {output_file}\")\n",
    "print(f\"  Contains {len(studies)} studies ready for nanopub generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export CSV and RIS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: wildfire-sentinel2-ml/screening_results/included_studies.csv (150 studies)\n",
      "✓ Saved: wildfire-sentinel2-ml/screening_results/excluded_studies.csv (110 studies)\n"
     ]
    }
   ],
   "source": [
    "# Export included studies to CSV\n",
    "export_cols = ['title', 'authors', 'year', 'doi', 'journal', 'abstract', 'url']\n",
    "available_export_cols = [c for c in export_cols if c in included_df.columns]\n",
    "\n",
    "included_df[available_export_cols].to_csv(\n",
    "    f\"{OUTPUT_DIR}/included_studies.csv\", \n",
    "    index=False\n",
    ")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/included_studies.csv ({len(included_df)} studies)\")\n",
    "\n",
    "# Export excluded studies to CSV\n",
    "excluded_df = merged_df[merged_df['status'] == 'excluded'].copy()\n",
    "excluded_df[available_export_cols].to_csv(\n",
    "    f\"{OUTPUT_DIR}/excluded_studies.csv\", \n",
    "    index=False\n",
    ")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/excluded_studies.csv ({len(excluded_df)} studies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: wildfire-sentinel2-ml/screening_results/included_studies.ris\n",
      "✓ Saved: wildfire-sentinel2-ml/screening_results/excluded_studies.ris\n"
     ]
    }
   ],
   "source": [
    "# Export to RIS format for reference managers\n",
    "def df_to_ris(df, filename):\n",
    "    \"\"\"Convert DataFrame to RIS format\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for idx, row in df.iterrows():\n",
    "            f.write(\"TY  - JOUR\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('title')):\n",
    "                f.write(f\"TI  - {row['title']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('authors')):\n",
    "                # Split authors if comma-separated\n",
    "                authors = str(row['authors'])\n",
    "                for author in authors.split(';'):\n",
    "                    author = author.strip()\n",
    "                    if author:\n",
    "                        f.write(f\"AU  - {author}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('year')):\n",
    "                f.write(f\"PY  - {int(row['year'])}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('journal')):\n",
    "                f.write(f\"JO  - {row['journal']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('doi')):\n",
    "                doi = row['doi']\n",
    "                if not doi.startswith('http'):\n",
    "                    doi = f\"https://doi.org/{doi}\"\n",
    "                f.write(f\"DO  - {row['doi']}\\n\")\n",
    "                f.write(f\"UR  - {doi}\\n\")\n",
    "            elif pd.notna(row.get('url')):\n",
    "                f.write(f\"UR  - {row['url']}\\n\")\n",
    "            \n",
    "            if pd.notna(row.get('abstract')):\n",
    "                # Clean abstract for RIS\n",
    "                abstract = str(row['abstract']).replace('\\n', ' ').strip()\n",
    "                f.write(f\"AB  - {abstract}\\n\")\n",
    "            \n",
    "            f.write(\"ER  - \\n\\n\")\n",
    "\n",
    "# Export included\n",
    "df_to_ris(included_df, f\"{OUTPUT_DIR}/included_studies.ris\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/included_studies.ris\")\n",
    "\n",
    "# Export excluded\n",
    "df_to_ris(excluded_df, f\"{OUTPUT_DIR}/excluded_studies.ris\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/excluded_studies.ris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temp directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"✓ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Review: Machine Learning Algorithms for Wildfire Detection and Burned Area Mapping using Sentinel-2\n",
      "Screener: Anne Fouilloux (0000-0002-1784-2920)\n",
      "\n",
      "Results:\n",
      "  Total records: 450\n",
      "  Screened: 260\n",
      "  Included: 150\n",
      "  Excluded: 110\n",
      "\n",
      "Output files in 'wildfire-sentinel2-ml/screening_results/' :\n",
      "  • study_inclusion.json     - For nanopub generation\n",
      "  • prisma_flow_data.json    - PRISMA diagram numbers\n",
      "  • included_studies.csv     - Included studies\n",
      "  • excluded_studies.csv     - Excluded studies\n",
      "  • included_studies.ris     - For Zotero/reference managers\n",
      "  • excluded_studies.ris     - For Zotero/reference managers\n",
      "\n",
      "Provenance chain:\n",
      "  PICO → Search Strategy → Search Execution → Study Inclusion\n",
      "============================================================\n",
      "\n",
      "Next step: Run study-inclusion-nanopub-from-json.ipynb\n",
      "           with study_inclusion.json to generate nanopubs\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nReview: {REVIEW_TITLE}\")\n",
    "print(f\"Screener: {SCREENER_NAME} ({SCREENER_ORCID})\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total records: {total_records}\")\n",
    "print(f\"  Screened: {screened}\")\n",
    "print(f\"  Included: {included}\")\n",
    "print(f\"  Excluded: {excluded}\")\n",
    "print(f\"\\nOutput files in '{OUTPUT_DIR}/' :\")\n",
    "print(f\"  • study_inclusion.json     - For nanopub generation\")\n",
    "print(f\"  • prisma_flow_data.json    - PRISMA diagram numbers\")\n",
    "print(f\"  • included_studies.csv     - Included studies\")\n",
    "print(f\"  • excluded_studies.csv     - Excluded studies\")\n",
    "print(f\"  • included_studies.ris     - For Zotero/reference managers\")\n",
    "print(f\"  • excluded_studies.ris     - For Zotero/reference managers\")\n",
    "print(f\"\\nProvenance chain:\")\n",
    "print(f\"  PICO → Search Strategy → Search Execution → Study Inclusion\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext step: Run study-inclusion-nanopub-from-json.ipynb\")\n",
    "print(\"           with study_inclusion.json to generate nanopubs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preview Study Inclusion JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of study_inclusion.json:\n",
      "\n",
      "{\n",
      "  \"review_metadata\": {\n",
      "    \"title\": \"Machine Learning Algorithms for Wildfire Detection and Burned Area Mapping using Sentinel-2\",\n",
      "    \"description\": \"Systematic review on Machine Learning Algorithms for Wildfire Detection and Burned Area Mapping using Sentinel-2\",\n",
      "    \"screener_orcid\": \"0000-0002-1784-2920\",\n",
      "    \"screener_name\": \"Anne Fouilloux\",\n",
      "    \"screening_date\": \"2026-01-08\",\n",
      "    \"screening_tool\": \"ASReview LAB v2.2\",\n",
      "    \"total_screened\": 260,\n",
      "    \"total_included\": 150,\n",
      "    \"total_excluded\": 110\n",
      "  },\n",
      "  \"provenance\": {\n",
      "    \"pico_nanopub\": \"https://w3id.org/np/RAjO8tdVOla9I77PeXF4iY92ULngrpx5_ZSKFkVrCmsW0\",\n",
      "    \"search_strategy_nanopub\": \" \",\n",
      "    \"search_execution_nanopub\": \" \"\n",
      "  },\n",
      "  \"studies\": [\n",
      "    {\n",
      "      \"uri\": \"urn:study:3142543\",\n",
      "      \"label\": \"FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Mario Markov\\\", \\\"Stefan Maria Ailuro\\\", \\\"Luc Van Gool\\\", \\\"Konrad Schindler\\\", \\\"Danda Pani Paudel\\\"]\",\n",
      "        \"year\": 2025,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": null,\n",
      "        \"abstract\": \"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"uri\": \"urn:study:945926\",\n",
      "      \"label\": \"Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Seyd Teymoor Seydi\\\"]\",\n",
      "        \"year\": 2025,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": null,\n",
      "        \"abstract\": \"This study presents a comprehensive analysis of four significant California wildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts through multiple dimensions, including land cover change, jurisdictional management, structural damage, and demographic vulnerability. Using the Chebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the extent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares. Our analysis revealed that shrubland ecosystems wer\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"uri\": \"https://doi.org/10.1109/IGARSS53475.2024.10641441\",\n",
      "      \"label\": \"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data\",\n",
      "      \"metadata\": {\n",
      "        \"authors\": \"[\\\"Yonghao Xu\\\", \\\"Amanda Berg\\\", \\\"Leif Haglund\\\"]\",\n",
      "        \"year\": 2024,\n",
      "        \"journal\": \"\",\n",
      "        \"doi\": \"10.1109/IGARSS53475.2024.10641441\",\n",
      "        \"abstract\": \"Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications. To advance the development of machine learning algorithms in this domain, our study introduces the \\\\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection. This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each patch has a size of 512$\\\\times$512 pixels wi\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "... and 147 more studies\n"
     ]
    }
   ],
   "source": [
    "# Show sample of the generated JSON\n",
    "print(\"Sample of study_inclusion.json:\\n\")\n",
    "preview = {\n",
    "    \"review_metadata\": study_inclusion_config[\"review_metadata\"],\n",
    "    \"provenance\": study_inclusion_config[\"provenance\"],\n",
    "    \"studies\": study_inclusion_config[\"studies\"][:3]  # First 3 studies\n",
    "}\n",
    "print(json.dumps(preview, indent=2, default=str))\n",
    "print(f\"\\n... and {len(studies) - 3} more studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
