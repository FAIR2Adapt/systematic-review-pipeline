{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Search Execution via APIs\n",
    "\n",
    "This notebook executes systematic review searches across multiple free academic databases using their APIs.\n",
    "\n",
    "**Supported Databases:**\n",
    "- OpenAlex (replaces Scopus/WoS)\n",
    "- arXiv\n",
    "- Semantic Scholar\n",
    "- PubMed (via Entrez)\n",
    "- Europe PMC\n",
    "\n",
    "**Note:** IEEE Xplore API requires institutional subscription - manual search recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install requests pyalex arxiv semanticscholar biopython -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyAlex available: True\n",
      "arXiv available: True\n",
      "Semantic Scholar available: True\n",
      "Entrez (PubMed) available: True\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# Optional imports - will check availability\n",
    "try:\n",
    "    from pyalex import Works, config as pyalex_config\n",
    "    PYALEX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYALEX_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import arxiv\n",
    "    ARXIV_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ARXIV_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from semanticscholar import SemanticScholar\n",
    "    S2_AVAILABLE = True\n",
    "except ImportError:\n",
    "    S2_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from Bio import Entrez\n",
    "    ENTREZ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ENTREZ_AVAILABLE = False\n",
    "\n",
    "print(f\"PyAlex available: {PYALEX_AVAILABLE}\")\n",
    "print(f\"arXiv available: {ARXIV_AVAILABLE}\")\n",
    "print(f\"Semantic Scholar available: {S2_AVAILABLE}\")\n",
    "print(f\"Entrez (PubMed) available: {ENTREZ_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit these settings for your search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== EDIT THIS SECTION ==============\n",
    "\n",
    "# Your email (required for polite API access)\n",
    "EMAIL = \"anne.fouilloux@gmail.com\"  # CHANGE THIS\n",
    "\n",
    "# Search parameters\n",
    "SEARCH_TERMS = {\n",
    "    \"quantum\": [\"quantum computing\", \"quantum machine learning\", \"QML\", \"QAOA\", \"quantum annealing\"],\n",
    "    \"biodiversity\": [\"biodiversity\", \"conservation\", \"species distribution\", \"ecological network\", \"population genetics\"]\n",
    "}\n",
    "\n",
    "# Date range\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2025\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"../search_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Maximum results to retrieve per database (set None for all)\n",
    "MAX_RESULTS = 500\n",
    "\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "search_results = {\n",
    "    \"search_date\": datetime.now().isoformat(),\n",
    "    \"databases\": {}\n",
    "}\n",
    "\n",
    "all_records = []  # Combined records for export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenAlex Search\n",
    "\n",
    "OpenAlex provides comprehensive bibliographic coverage as a free alternative to Scopus/WoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex: (quantum computing OR quantum machine learning OR QML OR QAOA OR quantum anneali...\n",
      "Total results available: 467\n",
      "Retrieved 467 records from OpenAlex\n"
     ]
    }
   ],
   "source": [
    "def search_openalex(search_terms, start_year, end_year, max_results=500, email=None):\n",
    "    \"\"\"Search OpenAlex using their REST API.\"\"\"\n",
    "    \n",
    "    # Build query: (quantum terms) AND (biodiversity terms)\n",
    "    quantum_query = \" OR \".join(search_terms[\"quantum\"])\n",
    "    bio_query = \" OR \".join(search_terms[\"biodiversity\"])\n",
    "    full_query = f\"({quantum_query}) AND ({bio_query})\"\n",
    "    \n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    \n",
    "    params = {\n",
    "        \"search\": full_query,\n",
    "        \"filter\": f\"publication_year:{start_year}-{end_year}\",\n",
    "        \"per_page\": 200,\n",
    "        \"cursor\": \"*\"\n",
    "    }\n",
    "    \n",
    "    if email:\n",
    "        params[\"mailto\"] = email\n",
    "    \n",
    "    results = []\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"Searching OpenAlex: {full_query[:80]}...\")\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        \n",
    "        if total_count == 0:\n",
    "            total_count = data.get(\"meta\", {}).get(\"count\", 0)\n",
    "            print(f\"Total results available: {total_count}\")\n",
    "        \n",
    "        works = data.get(\"results\", [])\n",
    "        if not works:\n",
    "            break\n",
    "            \n",
    "        for work in works:\n",
    "            # Safely extract journal name with multiple null checks\n",
    "            journal = \"\"\n",
    "            primary_loc = work.get(\"primary_location\")\n",
    "            if primary_loc:\n",
    "                source_obj = primary_loc.get(\"source\")\n",
    "                if source_obj:\n",
    "                    journal = source_obj.get(\"display_name\", \"\")\n",
    "            \n",
    "            record = {\n",
    "                \"source\": \"OpenAlex\",\n",
    "                \"id\": work.get(\"id\", \"\"),\n",
    "                \"doi\": work.get(\"doi\", \"\"),\n",
    "                \"title\": work.get(\"title\", \"\"),\n",
    "                \"year\": work.get(\"publication_year\"),\n",
    "                \"authors\": \"; \".join([name for name in [(a.get(\"author\") or {}).get(\"display_name\") for a in work.get(\"authorships\", [])] if name]),\n",
    "                \"journal\": journal,\n",
    "                \"abstract\": work.get(\"abstract\", \"\") or \"\",\n",
    "                \"type\": work.get(\"type\", \"\"),\n",
    "                \"is_oa\": work.get(\"open_access\", {}).get(\"is_oa\", False)\n",
    "            }\n",
    "            results.append(record)\n",
    "            \n",
    "        if max_results and len(results) >= max_results:\n",
    "            results = results[:max_results]\n",
    "            break\n",
    "            \n",
    "        # Get next page cursor\n",
    "        next_cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "        params[\"cursor\"] = next_cursor\n",
    "        \n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} records from OpenAlex\")\n",
    "    return results, total_count\n",
    "\n",
    "# Execute OpenAlex search\n",
    "openalex_results, openalex_total = search_openalex(SEARCH_TERMS, START_YEAR, END_YEAR, MAX_RESULTS, EMAIL)\n",
    "search_results[\"databases\"][\"OpenAlex\"] = {\n",
    "    \"total_available\": openalex_total,\n",
    "    \"retrieved\": len(openalex_results)\n",
    "}\n",
    "all_records.extend(openalex_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. arXiv Search\n",
    "\n",
    "Essential for quantum computing preprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv...\n",
      "Retrieved 178 records from arXiv\n"
     ]
    }
   ],
   "source": [
    "def search_arxiv(search_terms, start_year, end_year, max_results=500):\n",
    "    \"\"\"Search arXiv using their API.\"\"\"\n",
    "    \n",
    "    if not ARXIV_AVAILABLE:\n",
    "        print(\"arXiv package not available. Install with: pip install arxiv\")\n",
    "        return [], 0\n",
    "    \n",
    "    # Build query\n",
    "    quantum_terms = \" OR \".join([f'all:\"{t}\"' for t in search_terms[\"quantum\"]])\n",
    "    bio_terms = \" OR \".join([f'all:\"{t}\"' for t in search_terms[\"biodiversity\"]])\n",
    "    query = f\"({quantum_terms}) AND ({bio_terms})\"\n",
    "    \n",
    "    # Add category filter for relevant areas\n",
    "    categories = \"cat:quant-ph OR cat:cs.LG OR cat:cs.AI OR cat:q-bio*\"\n",
    "    \n",
    "    print(f\"Searching arXiv...\")\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results or 1000,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for paper in client.results(search):\n",
    "        pub_year = paper.published.year\n",
    "        if start_year <= pub_year <= end_year:\n",
    "            record = {\n",
    "                \"source\": \"arXiv\",\n",
    "                \"id\": paper.entry_id,\n",
    "                \"doi\": paper.doi or \"\",\n",
    "                \"title\": paper.title,\n",
    "                \"year\": pub_year,\n",
    "                \"authors\": \"; \".join([a.name for a in paper.authors]),\n",
    "                \"journal\": \"arXiv preprint\",\n",
    "                \"abstract\": paper.summary,\n",
    "                \"type\": \"preprint\",\n",
    "                \"is_oa\": True,\n",
    "                \"categories\": \", \".join(paper.categories)\n",
    "            }\n",
    "            results.append(record)\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} records from arXiv\")\n",
    "    return results, len(results)\n",
    "\n",
    "# Execute arXiv search\n",
    "arxiv_results, arxiv_total = search_arxiv(SEARCH_TERMS, START_YEAR, END_YEAR, MAX_RESULTS)\n",
    "search_results[\"databases\"][\"arXiv\"] = {\n",
    "    \"total_available\": arxiv_total,\n",
    "    \"retrieved\": len(arxiv_results)\n",
    "}\n",
    "all_records.extend(arxiv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PubMed Search\n",
    "\n",
    "Via NCBI Entrez E-utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed...\n",
      "Found 4 records, retrieving 4...\n",
      "Retrieved 4 records from PubMed\n"
     ]
    }
   ],
   "source": [
    "def search_pubmed(search_terms, start_year, end_year, max_results=500, email=None):\n",
    "    \"\"\"Search PubMed using Entrez E-utilities.\"\"\"\n",
    "    \n",
    "    if not ENTREZ_AVAILABLE:\n",
    "        print(\"Biopython not available. Install with: pip install biopython\")\n",
    "        return [], 0\n",
    "    \n",
    "    Entrez.email = email or \"user@example.com\"\n",
    "    \n",
    "    # Build PubMed query\n",
    "    quantum_terms = \" OR \".join([f'\"{t}\"[Title/Abstract]' for t in search_terms[\"quantum\"]])\n",
    "    bio_terms = \" OR \".join([f'\"{t}\"[Title/Abstract]' for t in search_terms[\"biodiversity\"]])\n",
    "    query = f\"({quantum_terms}) AND ({bio_terms}) AND ({start_year}:{end_year}[pdat])\"\n",
    "    \n",
    "    print(f\"Searching PubMed...\")\n",
    "    \n",
    "    # Search\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results or 10000)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    id_list = record[\"IdList\"]\n",
    "    total_count = int(record[\"Count\"])\n",
    "    \n",
    "    print(f\"Found {total_count} records, retrieving {len(id_list)}...\")\n",
    "    \n",
    "    if not id_list:\n",
    "        return [], total_count\n",
    "    \n",
    "    # Fetch details\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"xml\", retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    results = []\n",
    "    for article in records.get(\"PubmedArticle\", []):\n",
    "        medline = article.get(\"MedlineCitation\", {})\n",
    "        art = medline.get(\"Article\", {})\n",
    "        \n",
    "        # Get authors\n",
    "        authors = []\n",
    "        for author in art.get(\"AuthorList\", []):\n",
    "            if \"LastName\" in author:\n",
    "                name = f\"{author.get('LastName', '')} {author.get('ForeName', '')}\".strip()\n",
    "                authors.append(name)\n",
    "        \n",
    "        # Get abstract\n",
    "        abstract_parts = art.get(\"Abstract\", {}).get(\"AbstractText\", [])\n",
    "        abstract = \" \".join([str(p) for p in abstract_parts]) if abstract_parts else \"\"\n",
    "        \n",
    "        # Get year\n",
    "        pub_date = art.get(\"Journal\", {}).get(\"JournalIssue\", {}).get(\"PubDate\", {})\n",
    "        year = pub_date.get(\"Year\", \"\")\n",
    "        \n",
    "        # Get DOI\n",
    "        doi = \"\"\n",
    "        for eid in article.get(\"PubmedData\", {}).get(\"ArticleIdList\", []):\n",
    "            if eid.attributes.get(\"IdType\") == \"doi\":\n",
    "                doi = str(eid)\n",
    "                break\n",
    "        \n",
    "        record = {\n",
    "            \"source\": \"PubMed\",\n",
    "            \"id\": f\"PMID:{medline.get('PMID', '')}\",\n",
    "            \"doi\": doi,\n",
    "            \"title\": str(art.get(\"ArticleTitle\", \"\")),\n",
    "            \"year\": int(year) if year.isdigit() else None,\n",
    "            \"authors\": \"; \".join(authors),\n",
    "            \"journal\": art.get(\"Journal\", {}).get(\"Title\", \"\"),\n",
    "            \"abstract\": abstract,\n",
    "            \"type\": \"article\",\n",
    "            \"is_oa\": False  # Would need separate PMC check\n",
    "        }\n",
    "        results.append(record)\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} records from PubMed\")\n",
    "    return results, total_count\n",
    "\n",
    "# Execute PubMed search\n",
    "pubmed_results, pubmed_total = search_pubmed(SEARCH_TERMS, START_YEAR, END_YEAR, MAX_RESULTS, EMAIL)\n",
    "search_results[\"databases\"][\"PubMed\"] = {\n",
    "    \"total_available\": pubmed_total,\n",
    "    \"retrieved\": len(pubmed_results)\n",
    "}\n",
    "all_records.extend(pubmed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Europe PMC Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Europe PMC...\n",
      "Found 597 records\n",
      "Retrieved 500 records from Europe PMC\n"
     ]
    }
   ],
   "source": [
    "def search_europepmc(search_terms, start_year, end_year, max_results=500):\n",
    "    \"\"\"Search Europe PMC using their REST API.\"\"\"\n",
    "    \n",
    "    # Build query\n",
    "    quantum_query = \" OR \".join([f'\"{t}\"' for t in search_terms[\"quantum\"]])\n",
    "    bio_query = \" OR \".join([f'\"{t}\"' for t in search_terms[\"biodiversity\"]])\n",
    "    query = f\"({quantum_query}) AND ({bio_query}) AND (PUB_YEAR:[{start_year} TO {end_year}])\"\n",
    "    \n",
    "    base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"json\",\n",
    "        \"pageSize\": 1000,\n",
    "        \"resultType\": \"core\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Searching Europe PMC...\")\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return [], 0\n",
    "    \n",
    "    data = response.json()\n",
    "    total_count = data.get(\"hitCount\", 0)\n",
    "    \n",
    "    print(f\"Found {total_count} records\")\n",
    "    \n",
    "    results = []\n",
    "    for item in data.get(\"resultList\", {}).get(\"result\", [])[:max_results]:\n",
    "        record = {\n",
    "            \"source\": \"Europe PMC\",\n",
    "            \"id\": item.get(\"id\", \"\"),\n",
    "            \"doi\": item.get(\"doi\", \"\"),\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"year\": item.get(\"pubYear\"),\n",
    "            \"authors\": item.get(\"authorString\", \"\"),\n",
    "            \"journal\": item.get(\"journalTitle\", \"\"),\n",
    "            \"abstract\": item.get(\"abstractText\", \"\"),\n",
    "            \"type\": item.get(\"pubType\", \"\"),\n",
    "            \"is_oa\": item.get(\"isOpenAccess\", \"N\") == \"Y\"\n",
    "        }\n",
    "        results.append(record)\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} records from Europe PMC\")\n",
    "    return results, total_count\n",
    "\n",
    "# Execute Europe PMC search\n",
    "epmc_results, epmc_total = search_europepmc(SEARCH_TERMS, START_YEAR, END_YEAR, MAX_RESULTS)\n",
    "search_results[\"databases\"][\"Europe PMC\"] = {\n",
    "    \"total_available\": epmc_total,\n",
    "    \"retrieved\": len(epmc_results)\n",
    "}\n",
    "all_records.extend(epmc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Scholar Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Semantic Scholar...\n",
      "Rate limited, waiting...\n",
      "Total results available: 621\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Rate limited, waiting...\n",
      "Retrieved 500 records from Semantic Scholar\n"
     ]
    }
   ],
   "source": [
    "def search_semantic_scholar(search_terms, start_year, end_year, max_results=500):\n",
    "    \"\"\"Search Semantic Scholar using their API.\"\"\"\n",
    "    \n",
    "    # Build simple query (S2 works better with simpler queries)\n",
    "    query = \"quantum computing biodiversity conservation\"\n",
    "    \n",
    "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"year\": f\"{start_year}-{end_year}\",\n",
    "        \"fields\": \"paperId,externalIds,title,year,authors,venue,abstract,isOpenAccess\",\n",
    "        \"limit\": min(max_results or 100, 100)  # S2 limits to 100 per request\n",
    "    }\n",
    "    \n",
    "    print(f\"Searching Semantic Scholar...\")\n",
    "    \n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    results = []\n",
    "    offset = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    while True:\n",
    "        params[\"offset\"] = offset\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code == 429:\n",
    "            print(\"Rate limited, waiting...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "            \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if total_count == 0:\n",
    "            total_count = data.get(\"total\", 0)\n",
    "            print(f\"Total results available: {total_count}\")\n",
    "        \n",
    "        papers = data.get(\"data\", [])\n",
    "        if not papers:\n",
    "            break\n",
    "        \n",
    "        for paper in papers:\n",
    "            record = {\n",
    "                \"source\": \"Semantic Scholar\",\n",
    "                \"id\": paper.get(\"paperId\", \"\"),\n",
    "                \"doi\": paper.get(\"externalIds\", {}).get(\"DOI\", \"\"),\n",
    "                \"title\": paper.get(\"title\", \"\"),\n",
    "                \"year\": paper.get(\"year\"),\n",
    "                \"authors\": \"; \".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])]),\n",
    "                \"journal\": paper.get(\"venue\", \"\"),\n",
    "                \"abstract\": paper.get(\"abstract\", \"\") or \"\",\n",
    "                \"type\": \"article\",\n",
    "                \"is_oa\": paper.get(\"isOpenAccess\", False)\n",
    "            }\n",
    "            results.append(record)\n",
    "        \n",
    "        if max_results and len(results) >= max_results:\n",
    "            results = results[:max_results]\n",
    "            break\n",
    "            \n",
    "        offset += len(papers)\n",
    "        if offset >= total_count:\n",
    "            break\n",
    "            \n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} records from Semantic Scholar\")\n",
    "    return results, total_count\n",
    "\n",
    "# Execute Semantic Scholar search\n",
    "s2_results, s2_total = search_semantic_scholar(SEARCH_TERMS, START_YEAR, END_YEAR, MAX_RESULTS)\n",
    "search_results[\"databases\"][\"Semantic Scholar\"] = {\n",
    "    \"total_available\": s2_total,\n",
    "    \"retrieved\": len(s2_results)\n",
    "}\n",
    "all_records.extend(s2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEARCH RESULTS SUMMARY\n",
      "============================================================\n",
      "Search Date: 2025-12-26T19:05:32.698171\n",
      "Date Range: 2015-2025\n",
      "\n",
      "OpenAlex:\n",
      "  - Total available: 467\n",
      "  - Retrieved: 467\n",
      "arXiv:\n",
      "  - Total available: 178\n",
      "  - Retrieved: 178\n",
      "PubMed:\n",
      "  - Total available: 4\n",
      "  - Retrieved: 4\n",
      "Europe PMC:\n",
      "  - Total available: 597\n",
      "  - Retrieved: 500\n",
      "Semantic Scholar:\n",
      "  - Total available: 621\n",
      "  - Retrieved: 500\n",
      "\n",
      "TOTAL RECORDS (before deduplication): 1649\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEARCH RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Search Date: {search_results['search_date']}\")\n",
    "print(f\"Date Range: {START_YEAR}-{END_YEAR}\")\n",
    "print()\n",
    "\n",
    "total_retrieved = 0\n",
    "for db, counts in search_results[\"databases\"].items():\n",
    "    print(f\"{db}:\")\n",
    "    print(f\"  - Total available: {counts['total_available']}\")\n",
    "    print(f\"  - Retrieved: {counts['retrieved']}\")\n",
    "    total_retrieved += counts['retrieved']\n",
    "\n",
    "print()\n",
    "print(f\"TOTAL RECORDS (before deduplication): {total_retrieved}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1649 records to search_results/search_results_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "csv_path = OUTPUT_DIR / \"search_results_combined.csv\"\n",
    "\n",
    "if all_records:\n",
    "    fieldnames = [\"source\", \"id\", \"doi\", \"title\", \"year\", \"authors\", \"journal\", \"abstract\", \"type\", \"is_oa\"]\n",
    "    \n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    \n",
    "    print(f\"Exported {len(all_records)} records to {csv_path}\")\n",
    "else:\n",
    "    print(\"No records to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1649 records to search_results/search_results_combined.bib\n"
     ]
    }
   ],
   "source": [
    "# Export to BibTeX format\n",
    "def to_bibtex(records, filepath):\n",
    "    \"\"\"Convert records to BibTeX format.\"\"\"\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, rec in enumerate(records):\n",
    "            # Generate citation key\n",
    "            first_author = rec.get(\"authors\", \"Unknown\").split(\";\")[0].split()[-1] if rec.get(\"authors\") else \"Unknown\"\n",
    "            year = rec.get(\"year\", \"XXXX\")\n",
    "            key = f\"{first_author}{year}_{i}\"\n",
    "            \n",
    "            f.write(f\"@article{{{key},\\n\")\n",
    "            f.write(f\"  title = {{{rec.get('title', '')}}},\\n\")\n",
    "            f.write(f\"  author = {{{rec.get('authors', '')}}},\\n\")\n",
    "            f.write(f\"  year = {{{year}}},\\n\")\n",
    "            f.write(f\"  journal = {{{rec.get('journal', '')}}},\\n\")\n",
    "            if rec.get(\"doi\"):\n",
    "                f.write(f\"  doi = {{{rec.get('doi')}}},\\n\")\n",
    "            f.write(f\"  note = {{Source: {rec.get('source', '')}}},\\n\")\n",
    "            f.write(\"}\\n\\n\")\n",
    "    \n",
    "    print(f\"Exported {len(records)} records to {filepath}\")\n",
    "\n",
    "bibtex_path = OUTPUT_DIR / \"search_results_combined.bib\"\n",
    "if all_records:\n",
    "    to_bibtex(all_records, bibtex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1649 records to search_results/search_results_combined.ris\n"
     ]
    }
   ],
   "source": [
    "# Export to RIS format\n",
    "def to_ris(records, filepath):\n",
    "    \"\"\"Convert records to RIS format.\"\"\"\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(\"TY  - JOUR\\n\")\n",
    "            f.write(f\"TI  - {rec.get('title', '')}\\n\")\n",
    "            \n",
    "            for author in rec.get(\"authors\", \"\").split(\"; \"):\n",
    "                if author:\n",
    "                    f.write(f\"AU  - {author}\\n\")\n",
    "            \n",
    "            f.write(f\"PY  - {rec.get('year', '')}\\n\")\n",
    "            f.write(f\"JO  - {rec.get('journal', '')}\\n\")\n",
    "            \n",
    "            if rec.get(\"doi\"):\n",
    "                f.write(f\"DO  - {rec.get('doi')}\\n\")\n",
    "            \n",
    "            if rec.get(\"abstract\"):\n",
    "                f.write(f\"AB  - {rec.get('abstract', '')[:1000]}\\n\")\n",
    "            \n",
    "            f.write(f\"N1  - Source: {rec.get('source', '')}\\n\")\n",
    "            f.write(\"ER  - \\n\\n\")\n",
    "    \n",
    "    print(f\"Exported {len(records)} records to {filepath}\")\n",
    "\n",
    "ris_path = OUTPUT_DIR / \"search_results_combined.ris\"\n",
    "if all_records:\n",
    "    to_ris(all_records, ris_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved search summary to search_results/search_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Save search summary JSON\n",
    "summary_path = OUTPUT_DIR / \"search_summary.json\"\n",
    "\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved search summary to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEDUPLICATION PREVIEW (DOI-based):\n",
      "  Total records: 1649\n",
      "  Duplicates removed: 33\n",
      "  Records without DOI: 240\n",
      "  Unique records: 1616\n",
      "\n",
      "Note: Full deduplication should be done in Zotero/Rayyan with title matching\n"
     ]
    }
   ],
   "source": [
    "# Simple deduplication based on DOI\n",
    "def deduplicate_by_doi(records):\n",
    "    \"\"\"Remove duplicates based on DOI.\"\"\"\n",
    "    seen_dois = set()\n",
    "    unique = []\n",
    "    duplicates = 0\n",
    "    no_doi = 0\n",
    "    \n",
    "    for rec in records:\n",
    "        doi = (rec.get(\"doi\") or \"\").strip().lower()\n",
    "        if not doi:\n",
    "            no_doi += 1\n",
    "            unique.append(rec)  # Keep records without DOI\n",
    "        elif doi not in seen_dois:\n",
    "            seen_dois.add(doi)\n",
    "            unique.append(rec)\n",
    "        else:\n",
    "            duplicates += 1\n",
    "    \n",
    "    return unique, duplicates, no_doi\n",
    "\n",
    "unique_records, dup_count, no_doi_count = deduplicate_by_doi(all_records)\n",
    "\n",
    "print(f\"\\nDEDUPLICATION PREVIEW (DOI-based):\")\n",
    "print(f\"  Total records: {len(all_records)}\")\n",
    "print(f\"  Duplicates removed: {dup_count}\")\n",
    "print(f\"  Records without DOI: {no_doi_count}\")\n",
    "print(f\"  Unique records: {len(unique_records)}\")\n",
    "print(\"\\nNote: Full deduplication should be done in Zotero/Rayyan with title matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Search Execution JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON saved to: search-execution-quantum-biodiversity_updated.json\n"
     ]
    }
   ],
   "source": [
    "# Path to your search execution JSON file\n",
    "JSON_CONFIG_PATH = \"../inputs/search-execution-quantum-biodiversity.json\"  # Update path as needed\n",
    "\n",
    "def update_search_execution_json(json_path, search_results, unique_count):\n",
    "    \"\"\"Update the search execution JSON with actual results.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"JSON file not found: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    # Update creation date\n",
    "    config[\"search_execution_dataset\"][\"creation_date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Update database counts\n",
    "    db_mapping = {\n",
    "        \"OpenAlex\": \"openalex.org\",\n",
    "        \"arXiv\": \"arxiv.org\",\n",
    "        \"Semantic Scholar\": \"semanticscholar.org\",\n",
    "        \"PubMed\": \"pubmed.ncbi.nlm.nih.gov\",\n",
    "        \"Europe PMC\": \"europepmc.org\"\n",
    "    }\n",
    "    \n",
    "    for db_search in config[\"search_execution_dataset\"][\"db_searches\"]:\n",
    "        for db_name, url_part in db_mapping.items():\n",
    "            if url_part in db_search[\"database_url\"]:\n",
    "                if db_name in search_results[\"databases\"]:\n",
    "                    db_search[\"results_count\"] = search_results[\"databases\"][db_name][\"total_available\"]\n",
    "                break\n",
    "    \n",
    "    # Update screening counts (placeholder - actual screening happens later)\n",
    "    total = sum(d[\"total_available\"] for d in search_results[\"databases\"].values())\n",
    "    config[\"search_execution_dataset\"][\"screened_record_count\"] = str(unique_count)\n",
    "    \n",
    "    # Save updated JSON\n",
    "    updated_path = json_path.replace(\".json\", \"_updated.json\")\n",
    "    with open(updated_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Updated JSON saved to: {updated_path}\")\n",
    "\n",
    "# Uncomment to update your JSON file:\n",
    "update_search_execution_json(JSON_CONFIG_PATH, search_results, len(unique_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Import to reference manager**: Load `search_results_combined.ris` into Zotero or Rayyan\n",
    "2. **Full deduplication**: Use Zotero's duplicate detection or Rayyan's deduplication\n",
    "3. **Title/Abstract screening**: Apply inclusion/exclusion criteria\n",
    "4. **Full-text screening**: Review remaining candidates\n",
    "5. **Update counts**: Fill in final numbers in search execution JSON\n",
    "6. **Generate nanopub**: Run search-execution-nanopub-from-json.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
