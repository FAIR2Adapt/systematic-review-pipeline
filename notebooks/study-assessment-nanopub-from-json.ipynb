{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRISMA Study Assessment Dataset Nanopublication Generator\n",
    "\n",
    "This notebook generates a nanopublication declaring a PRISMA study assessment dataset for a systematic review.\n",
    "\n",
    "**Template:** [Declaring a PRISMA study assessment dataset](https://w3id.org/np/RAwQj3SNiopwPrHXfoRT2JtYZSt-5JsDHjBDW6nYz_rDE) by Anne Fouilloux & Tobias Kuhn (2025-11-24)\n",
    "\n",
    "## PRISMA 2020 Items Covered\n",
    "- **Item 5:** Eligibility criteria\n",
    "- **Item 11:** Risk of bias assessment methods\n",
    "- **Item 17:** Study characteristics\n",
    "- **Items 18-19:** Individual study results\n",
    "\n",
    "## Workflow Position\n",
    "1. PICO Research Question ✅\n",
    "2. Search Strategy ✅\n",
    "3. Search Execution Dataset ✅\n",
    "4. Study Inclusion ✅ (one per study)\n",
    "5. **Study Assessment Dataset** ← This notebook (one per review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Set the path to your JSON configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your JSON configuration file\n",
    "ASSESSMENT_FILE = \"../inputs/study-assessment-quantum-biodiversity.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "\n",
      "Author: Anne Fouilloux (0000-0002-1784-2920)\n",
      "Systematic Review: https://w3id.org/np/RA8B3ptXUOsN7obpkFGtA0FBmsh0OnID53wOsUIpSKTcg\n",
      "\n",
      "Assessment Dataset: Study Assessment Dataset: Quantum Computing Applications in Biodiversity Research\n",
      "Creation Date: 2025-12-27\n",
      "Dataset Location: https://zenodo.org/records/18070378\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(ASSESSMENT_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"\\nAuthor: {config['author']['name']} ({config['author']['orcid']})\")\n",
    "print(f\"Systematic Review: {config['systematic_review']}\")\n",
    "print(f\"\\nAssessment Dataset: {config['assessment']['label']}\")\n",
    "print(f\"Creation Date: {config['assessment']['creation_date']}\")\n",
    "print(f\"Dataset Location: {config['assessment']['dataset_file_location']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Namespaces and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces configured.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from rdflib import Graph, Dataset, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD, FOAF, PROV\n",
    "\n",
    "# Define namespaces (DCT as Namespace to allow custom predicates)\n",
    "NP = Namespace(\"http://www.nanopub.org/nschema#\")\n",
    "NPX = Namespace(\"http://purl.org/nanopub/x/\")\n",
    "NT = Namespace(\"https://w3id.org/np/o/ntemplate/\")\n",
    "ORCID = Namespace(\"https://orcid.org/\")\n",
    "SLV = Namespace(\"https://w3id.org/sciencelive/o/terms/\")\n",
    "DCT = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "# Temporary namespace for building\n",
    "TEMP_NP = Namespace(\"http://purl.org/nanopub/temp/np/\")\n",
    "\n",
    "# Template URIs\n",
    "STUDY_ASSESSMENT_TEMPLATE = URIRef(\"https://w3id.org/np/RAwQj3SNiopwPrHXfoRT2JtYZSt-5JsDHjBDW6nYz_rDE\")\n",
    "PROVENANCE_TEMPLATE = URIRef(\"https://w3id.org/np/RA7lSq6MuK_TIC6JMSHvLtee3lpLoZDOqLJCLXevnrPoU\")\n",
    "PUBINFO_TEMPLATE_1 = URIRef(\"https://w3id.org/np/RA0J4vUn_dekg-U1kK3AOEt02p9mT2WO03uGxLDec1jLw\")\n",
    "PUBINFO_TEMPLATE_2 = URIRef(\"https://w3id.org/np/RAukAcWHRDlkqxk7H2XNSegc1WnHI569INvNr-xdptDGI\")\n",
    "\n",
    "print(\"Namespaces configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Nanopublication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and URIs created.\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset with namespace bindings\n",
    "ds = Dataset()\n",
    "ds.bind(\"this\", \"http://purl.org/nanopub/temp/np/\")\n",
    "ds.bind(\"sub\", TEMP_NP)\n",
    "ds.bind(\"np\", NP)\n",
    "ds.bind(\"dct\", DCT)\n",
    "ds.bind(\"nt\", NT)\n",
    "ds.bind(\"npx\", NPX)\n",
    "ds.bind(\"xsd\", XSD)\n",
    "ds.bind(\"rdfs\", RDFS)\n",
    "ds.bind(\"orcid\", ORCID)\n",
    "ds.bind(\"prov\", PROV)\n",
    "ds.bind(\"foaf\", FOAF)\n",
    "ds.bind(\"slv\", SLV)\n",
    "\n",
    "# URIs\n",
    "np_uri = URIRef(\"http://purl.org/nanopub/temp/np/\")\n",
    "head_uri = TEMP_NP.Head\n",
    "assertion_uri = TEMP_NP.assertion\n",
    "provenance_uri = TEMP_NP.provenance\n",
    "pubinfo_uri = TEMP_NP.pubinfo\n",
    "dataset_uri = TEMP_NP.studyAssessmentDataset\n",
    "author_uri = ORCID[config['author']['orcid']]\n",
    "systematic_review_uri = URIRef(config['systematic_review'])\n",
    "\n",
    "print(\"Dataset and URIs created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD graph created.\n"
     ]
    }
   ],
   "source": [
    "# HEAD graph\n",
    "head = ds.graph(head_uri)\n",
    "head.add((np_uri, RDF.type, NP.Nanopublication))\n",
    "head.add((np_uri, NP.hasAssertion, assertion_uri))\n",
    "head.add((np_uri, NP.hasProvenance, provenance_uri))\n",
    "head.add((np_uri, NP.hasPublicationInfo, pubinfo_uri))\n",
    "\n",
    "print(\"HEAD graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSERTION graph created with 12 triples.\n"
     ]
    }
   ],
   "source": [
    "# ASSERTION graph\n",
    "assertion = ds.graph(assertion_uri)\n",
    "assessment = config['assessment']\n",
    "\n",
    "# Type declaration\n",
    "assertion.add((dataset_uri, RDF.type, SLV.StudyAssessmentDataset))\n",
    "\n",
    "# Label\n",
    "assertion.add((dataset_uri, RDFS.label, Literal(assessment['label'])))\n",
    "\n",
    "# Link to systematic review\n",
    "assertion.add((dataset_uri, DCT.isPartOf, systematic_review_uri))\n",
    "\n",
    "# Creation date\n",
    "assertion.add((dataset_uri, DCT.created, Literal(assessment['creation_date'], datatype=XSD.date)))\n",
    "\n",
    "# Eligibility criteria (PRISMA Item 5)\n",
    "assertion.add((dataset_uri, SLV.followsEligibilityCriteria, Literal(assessment['eligibility_criteria'])))\n",
    "\n",
    "# Assessment technique (PRISMA Item 11)\n",
    "assertion.add((dataset_uri, SLV.usesAssessmentTechnique, Literal(assessment['assessment_technique'])))\n",
    "\n",
    "# Study characteristics (PRISMA Item 17)\n",
    "assertion.add((dataset_uri, SLV.hasStudyCharacteristics, Literal(assessment['study_characteristics'])))\n",
    "\n",
    "# Extraction method\n",
    "assertion.add((dataset_uri, DCT.hasExtractionMethod, Literal(assessment['extraction_method'])))\n",
    "\n",
    "# Study results (PRISMA Item 19)\n",
    "assertion.add((dataset_uri, DCT.hasStudyResults, Literal(assessment['study_results'])))\n",
    "\n",
    "# Quality assessment (PRISMA Item 18)\n",
    "assertion.add((dataset_uri, DCT.hasQualityAssessment, Literal(assessment['quality_assessment'])))\n",
    "\n",
    "# Dataset file location\n",
    "assertion.add((dataset_uri, SLV.hasDatasetFileLocation, URIRef(assessment['dataset_file_location'])))\n",
    "\n",
    "# Limitations (optional)\n",
    "if assessment.get('limitations'):\n",
    "    assertion.add((dataset_uri, SLV.hasLimitations, Literal(assessment['limitations'])))\n",
    "\n",
    "print(f\"ASSERTION graph created with {len(assertion)} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROVENANCE graph created.\n"
     ]
    }
   ],
   "source": [
    "# PROVENANCE graph\n",
    "provenance = ds.graph(provenance_uri)\n",
    "provenance.add((assertion_uri, PROV.wasAttributedTo, author_uri))\n",
    "\n",
    "print(\"PROVENANCE graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBINFO graph created.\n"
     ]
    }
   ],
   "source": [
    "# PUBINFO graph\n",
    "pubinfo = ds.graph(pubinfo_uri)\n",
    "\n",
    "# Author info\n",
    "pubinfo.add((author_uri, FOAF.name, Literal(config['author']['name'])))\n",
    "\n",
    "# Timestamp\n",
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "pubinfo.add((np_uri, DCT.created, Literal(timestamp, datatype=XSD.dateTime)))\n",
    "\n",
    "# Creator\n",
    "pubinfo.add((np_uri, DCT.creator, author_uri))\n",
    "\n",
    "# License\n",
    "pubinfo.add((np_uri, DCT.license, URIRef(\"https://creativecommons.org/licenses/by/4.0/\")))\n",
    "\n",
    "# Introduces the dataset\n",
    "pubinfo.add((np_uri, NPX.introduces, dataset_uri))\n",
    "\n",
    "# Created at\n",
    "pubinfo.add((np_uri, NPX.wasCreatedAt, URIRef(\"https://nanodash.knowledgepixels.com/\")))\n",
    "\n",
    "# Label (truncated to 50 chars)\n",
    "label = assessment['label'][:50] + \"...\" if len(assessment['label']) > 50 else assessment['label']\n",
    "pubinfo.add((np_uri, RDFS.label, Literal(label)))\n",
    "\n",
    "# Template references\n",
    "pubinfo.add((np_uri, NT.wasCreatedFromTemplate, STUDY_ASSESSMENT_TEMPLATE))\n",
    "pubinfo.add((np_uri, NT.wasCreatedFromProvenanceTemplate, PROVENANCE_TEMPLATE))\n",
    "pubinfo.add((np_uri, NT.wasCreatedFromPubinfoTemplate, PUBINFO_TEMPLATE_1))\n",
    "pubinfo.add((np_uri, NT.wasCreatedFromPubinfoTemplate, PUBINFO_TEMPLATE_2))\n",
    "\n",
    "print(\"PUBINFO graph created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Serialize and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nanopub saved to: quantum-biodiversity-study-assessment.trig\n",
      "\n",
      "Assertion contains 12 triples\n"
     ]
    }
   ],
   "source": [
    "# Serialize to TriG\n",
    "trig_output = ds.serialize(format='trig')\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(config['output']['filename'])\n",
    "output_path.write_text(trig_output)\n",
    "\n",
    "print(f\"✅ Nanopub saved to: {output_path}\")\n",
    "print(f\"\\nAssertion contains {len(assertion)} triples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated TriG output:\n",
      "======================================================================\n",
      "@prefix dct: <http://purl.org/dc/terms/> .\n",
      "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
      "@prefix np: <http://www.nanopub.org/nschema#> .\n",
      "@prefix npx: <http://purl.org/nanopub/x/> .\n",
      "@prefix nt: <https://w3id.org/np/o/ntemplate/> .\n",
      "@prefix orcid: <https://orcid.org/> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix slv: <https://w3id.org/sciencelive/o/terms/> .\n",
      "@prefix sub: <http://purl.org/nanopub/temp/np/> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "sub:provenance {\n",
      "    sub:assertion prov:wasAttributedTo orcid:0000-0002-1784-2920 .\n",
      "}\n",
      "\n",
      "sub:pubinfo {\n",
      "    sub: rdfs:label \"Study Assessment Dataset: Quantum Computing Applic...\" ;\n",
      "        dct:created \"2025-12-27T18:01:06.231000+00:00\"^^xsd:dateTime ;\n",
      "        dct:creator orcid:0000-0002-1784-2920 ;\n",
      "        dct:license <https://creativecommons.org/licenses/by/4.0/> ;\n",
      "        npx:introduces sub:studyAssessmentDataset ;\n",
      "        npx:wasCreatedAt <https://nanodash.knowledgepixels.com/> ;\n",
      "        nt:wasCreatedFromProvenanceTemplate <https://w3id.org/np/RA7lSq6MuK_TIC6JMSHvLtee3lpLoZDOqLJCLXevnrPoU> ;\n",
      "        nt:wasCreatedFromPubinfoTemplate <https://w3id.org/np/RA0J4vUn_dekg-U1kK3AOEt02p9mT2WO03uGxLDec1jLw>,\n",
      "            <https://w3id.org/np/RAukAcWHRDlkqxk7H2XNSegc1WnHI569INvNr-xdptDGI> ;\n",
      "        nt:wasCreatedFromTemplate <https://w3id.org/np/RAwQj3SNiopwPrHXfoRT2JtYZSt-5JsDHjBDW6nYz_rDE> .\n",
      "\n",
      "    orcid:0000-0002-1784-2920 foaf:name \"Anne Fouilloux\" .\n",
      "}\n",
      "\n",
      "sub:assertion {\n",
      "    sub:studyAssessmentDataset a slv:StudyAssessmentDataset ;\n",
      "        rdfs:label \"Study Assessment Dataset: Quantum Computing Applications in Biodiversity Research\" ;\n",
      "        dct:created \"2025-12-27\"^^xsd:date ;\n",
      "        dct:hasExtractionMethod \"Automated extraction from academic database APIs with manual verification. Data extracted: Title, Authors, Year, DOI, Abstract, Journal/Source. Tools used: OpenAlex API, arXiv API, PubMed E-utilities, Europe PMC API, Semantic Scholar API, Zotero reference manager.\" ;\n",
      "        dct:hasQualityAssessment \"AI-assisted prioritization with subjective human decision-making. ASReview LAB uses active learning to prioritize papers most likely to be relevant, while human reviewer makes all inclusion/exclusion decisions based on subjective assessment of potential relevance. Bias considerations: Single screener's subjective interpretation of 'potential relevance'; Active learning may miss relevant papers in unexplored regions of feature space; No inter-rater reliability assessment; English language bias; Inclusive approach may result in heterogeneous study population.\" ;\n",
      "        dct:hasStudyResults \"283 studies identified as potentially relevant to quantum computing applications in biodiversity research. 238 studies with DOIs published as individual Study Inclusion nanopublications. Studies span quantum algorithms, quantum machine learning, ecological modeling, species identification, and conservation optimization.\" ;\n",
      "        dct:isPartOf <https://w3id.org/np/RA8B3ptXUOsN7obpkFGtA0FBmsh0OnID53wOsUIpSKTcg> ;\n",
      "        slv:followsEligibilityCriteria \"Exploratory scoping review with inclusive screening. INCLUSION: Studies applying quantum computing methods to biodiversity or ecological research; Studies on biodiversity/ecology topics that could potentially benefit from quantum computing approaches; Studies on quantum algorithms relevant to environmental modeling, optimization, or data analysis; Peer-reviewed articles, preprints, and conference papers; Publications from 2015-2025; English language publications. EXCLUSION: Clearly off-topic papers (neither quantum computing nor biodiversity related); Conference abstracts only; Non-English publications; Retracted papers. NOTE: Screening used subjective judgment to identify papers with potential relevance to the intersection of quantum computing and biodiversity, including biodiversity studies that might benefit from quantum approaches even without explicit quantum computing content.\" ;\n",
      "        slv:hasDatasetFileLocation <https://zenodo.org/records/18070378> ;\n",
      "        slv:hasLimitations \"Single screener with subjective judgment - no independent verification. Inclusive screening approach: some included papers may not directly address quantum computing. AI-assisted screening may have missed relevant studies in unscreened portion. Limited to English language publications. Quantum computing and biodiversity are rapidly evolving fields - findings may become outdated. Interdisciplinary nature makes comprehensive coverage challenging. 45 included studies lacked DOIs and could not be published as nanopubs.\" ;\n",
      "        slv:hasStudyCharacteristics \"Time range: 2015-2025. Databases searched: OpenAlex, arXiv, PubMed, Europe PMC, Semantic Scholar. Domains: Quantum computing, Quantum machine learning, Biodiversity, Conservation biology, Ecological modeling, Species distribution modeling. Study types: Original research, Method development, Application studies, Computational experiments. Total records: 1649. Records screened: 569. Records included: 283. Records excluded: 286. Not screened (AI stopped): 1080. Nanopubs published: 238 (45 skipped due to missing DOIs).\" ;\n",
      "        slv:usesAssessmentTechnique \"AI-assisted systematic screening using active learning. Tool: ASReview LAB v2.2 (https://asreview.nl/). Model: Naive Bayes classifier with TF-IDF feature extraction. Screening type: Title and abstract screening. Screeners: 1. Stopping rule: Consecutive irrelevant threshold - stopped after sustained pattern of irrelevant papers. Estimated recall: >95%.\" .\n",
      "}\n",
      "\n",
      "sub:Head {\n",
      "    sub: a np:Nanopublication ;\n",
      "        np:hasAssertion sub:assertion ;\n",
      "        np:hasProvenance sub:provenance ;\n",
      "        np:hasPublicationInfo sub:pubinfo .\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated TriG output:\")\n",
    "print(\"=\" * 70)\n",
    "print(trig_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation passed!\n",
      "   Assertion triples: 12\n",
      "   Provenance triples: 1\n",
      "   Pubinfo triples: 11\n"
     ]
    }
   ],
   "source": [
    "from nanopub import Nanopub, NanopubConf\n",
    "\n",
    "conf = NanopubConf(use_test_server=True)\n",
    "\n",
    "try:\n",
    "    np_obj = Nanopub(rdf=output_path, conf=conf)\n",
    "    print(f\"✅ Validation passed!\")\n",
    "    print(f\"   Assertion triples: {len(list(np_obj.assertion))}\")\n",
    "    print(f\"   Provenance triples: {len(list(np_obj.provenance))}\")\n",
    "    print(f\"   Pubinfo triples: {len(list(np_obj.pubinfo))}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Sign and Publish (Optional)\n",
    "\n",
    "⚠️ **Warning:** Uncomment only when ready to publish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded profile: Anne Fouilloux\n",
      "✓ Signed\n",
      "✅ Published: https://w3id.org/np/RAx_ZQScbvsz7Rvqk8scSYx06zojCc6Gjcvkxjj_MKwVM\n"
     ]
    }
   ],
   "source": [
    "# Sign and publish\n",
    "PUBLISH = True\n",
    "USE_TEST_SERVER = False\n",
    "\n",
    "if PUBLISH:\n",
    "    from nanopub import Nanopub, NanopubConf, load_profile\n",
    "    \n",
    "    profile = load_profile()\n",
    "    print(f\"Loaded profile: {profile.name}\")\n",
    "    \n",
    "    conf = NanopubConf(profile=profile, use_test_server=USE_TEST_SERVER)\n",
    "    np_obj = Nanopub(rdf=output_path, conf=conf)\n",
    "    \n",
    "    np_obj.sign()\n",
    "    print(f\"✓ Signed\")\n",
    "    \n",
    "    np_obj.publish()\n",
    "    print(f\"✅ Published: {np_obj.source_uri}\")\n",
    "else:\n",
    "    print(\"Publishing disabled. Set PUBLISH = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## JSON Configuration Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"author\": {\n",
    "    \"orcid\": \"0000-0002-1784-2920\",\n",
    "    \"name\": \"Anne Fouilloux\"\n",
    "  },\n",
    "  \"systematic_review\": \"https://w3id.org/np/...\",\n",
    "  \"assessment\": {\n",
    "    \"label\": \"Study Assessment Dataset Label\",\n",
    "    \"creation_date\": \"2025-01-15\",\n",
    "    \"eligibility_criteria\": \"PRISMA Item 5: Detailed inclusion/exclusion criteria...\",\n",
    "    \"assessment_technique\": \"PRISMA Item 11: Risk of bias tools and methods...\",\n",
    "    \"study_characteristics\": \"PRISMA Item 17: Summary across included studies...\",\n",
    "    \"extraction_method\": \"Data extraction methodology...\",\n",
    "    \"study_results\": \"PRISMA Item 19: Individual study results...\",\n",
    "    \"quality_assessment\": \"PRISMA Item 18: Risk of bias results...\",\n",
    "    \"dataset_file_location\": \"https://zenodo.org/records/...\",\n",
    "    \"limitations\": \"Optional: Assessment limitations...\"\n",
    "  },\n",
    "  \"output\": {\n",
    "    \"filename\": \"my-study-assessment.trig\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## PRISMA 2020 Mapping\n",
    "\n",
    "| JSON Field | PRISMA Item | Description |\n",
    "|------------|-------------|-------------|\n",
    "| `eligibility_criteria` | Item 5 | Inclusion and exclusion criteria |\n",
    "| `assessment_technique` | Item 11 | Risk of bias assessment methods |\n",
    "| `study_characteristics` | Item 17 | Characteristics of included studies |\n",
    "| `quality_assessment` | Item 18 | Risk of bias in individual studies |\n",
    "| `study_results` | Item 19 | Results of individual studies |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
