{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289f09ff-ce20-4a7a-a5c1-3c9effea60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ SCIENCE LIVE - Systematic Review Screening Pipeline\n",
      "============================================================\n",
      "Project folder: /Users/annef/Documents/FAIR2Adapt/systematic-review-pipeline/notebooks/wildfire-sentinel2-ml\n",
      "Checkpoint: wildfire-sentinel2-ml/scanning_results/checkpoint.jsonl\n",
      "Debug responses: wildfire-sentinel2-ml/debug (for uncertain parses)\n",
      "============================================================\n",
      "STEP 1: Loading papers from Zenodo CSV\n",
      "============================================================\n",
      "Loaded 7 papers from pets-biodiversity/screening_results/included_studies.csv\n",
      "  Missing abstracts: 7\n",
      "Enriching from OpenAlex...\n",
      "  Enriched 7 papers\n",
      "\n",
      "Summary:\n",
      "  Total papers: 7\n",
      "  With DOI: 7\n",
      "  With abstract: 7\n",
      "\n",
      "============================================================\n",
      "STEP 3: Screening papers against PICO\n",
      "============================================================\n",
      "\n",
      "üìÇ Existing checkpoint found:\n",
      "   Already screened: 150\n",
      "   INCLUDE: 73, EXCLUDE: 77, ERROR: 0\n",
      "   Remaining: -143\n",
      "Fetching PICO from: https://w3id.org/np/RAqmVeNbWgL7sNtsr9GqdX0ZTa6aQf3itQmort-JMy4tM\n",
      "‚úì Loaded: Privacy-Enhancing Technologies for Geospatial Biodiversity Data Sharing: A Scoping Review\n",
      "\n",
      "PICO Research Question\n",
      "======================\n",
      "Title: Privacy-Enhancing Technologies for Geospatial Biodiversity Data Sharing: A Scoping Review\n",
      "URI: https://w3id.org/np/RAqmVeNbWgL7sNtsr9GqdX0ZTa6aQf3itQmort-JMy4tM\n",
      "\n",
      "Question: In geospatial species occurrence datasets, how do different privacy-enhancing technologies compare to each other and to unprotected sharing in terms of privacy guarantee strength, preservation of ecological analytical utility, and implementation feasibility for biodiversity monitoring organizations?\n",
      "\n",
      "Population: Geospatial species occurrence datasets shared by biodiversity monitoring organizations, including citizen science platforms, national biodiversity databases, GBIF-mediated data, and sensitive species location records used in conservation planning and ecological research\n",
      "Intervention: Privacy-enhancing technologies including spatial generalization (grid-based aggregation, convex hulls), differential privacy mechanisms, controlled access systems (tiered permissions, data use agreements), spatial cloaking, k-anonymity adaptations for location data, and emerging approaches such as secure multi-party computation and zero-knowledge proofs for data governance\n",
      "Comparator: Alternative PET approaches compared against each other, against unprotected data sharing baselines, and across different parameterizations (e.g., grid resolutions, epsilon values, access tier structures)\n",
      "Outcome: Privacy protection effectiveness (re-identification risk, formal privacy guarantees), preservation of ecological analytical utility (species distribution model accuracy, biodiversity index reliability, spatial pattern retention), implementation feasibility (computational cost, regulatory compliance, organizational adoption barriers), and documented trade-offs between privacy and utility\n",
      "\n",
      "Creator: Anne Fouilloux (https://orcid.org/0000-0002-1784-2920)\n",
      "üìÇ Checkpoint found: 138 DOIs + 12 titles already screened\n",
      "[1/7] Screening: Ethics of Environmental and Biodiversity Data. Whe... ‚úó EXCLUDE (0.90): The paper focuses on ethical and governance aspect\n",
      "[2/7] Screening: An Efficient Approach Based on Privacy-Preserving ... ‚úó EXCLUDE (0.90): The paper focuses on privacy-preserving deep learn\n",
      "[3/7] Screening: Ethics of Environmental and Biodiversity Data. Whe... ‚úó EXCLUDE (0.90): The paper focuses on ethical and governance aspect\n",
      "[4/7] Screening: Ethics of Environmental and Biodiversity Data. Whe... ‚úó EXCLUDE (0.90): The paper focuses on ethical and governance aspect\n",
      "[5/7] Screening: Exploring the Methods and the Need for Data Anonym... ‚úó EXCLUDE (0.40): Insufficient evidence for inclusion (low confidenc\n",
      "[6/7] Screening: Ethical Challenges in AI-Driven Soundscape Monitor... ‚úó EXCLUDE (0.90): The paper focuses on ethical challenges and techni\n",
      "[7/7] Screening: Ethical Challenges in AI-Driven Soundscape Monitor... ‚úó EXCLUDE (0.90): The paper focuses on ethical challenges and techni\n",
      "\n",
      "============================================================\n",
      "STEP 4: Exporting results\n",
      "============================================================\n",
      "  Saved: wildfire-sentinel2-ml/scanning_results/screening_results.json\n",
      "  Saved: wildfire-sentinel2-ml/scanning_results/screening_results.csv\n",
      "  Saved: wildfire-sentinel2-ml/scanning_results/included_papers.csv\n",
      "  Saved: wildfire-sentinel2-ml/scanning_results/excluded_papers.csv\n",
      "\n",
      "üìä SCREENING SUMMARY:\n",
      "   Total screened: 157\n",
      "   INCLUDE: 73 (46.5%)\n",
      "   EXCLUDE: 84 (53.5%)\n",
      "\n",
      "   Exclusion reasons:\n",
      "     E3: 26\n",
      "     E4: 25\n",
      "     E2: 23\n",
      "     E1: 10\n",
      "\n",
      "‚úÖ Pipeline complete!\n",
      "   Results in: /Users/annef/Documents/FAIR2Adapt/systematic-review-pipeline/notebooks/wildfire-sentinel2-ml/scanning_results\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Science Live - Zenodo CSV Screening Example\n",
    "\n",
    "This script demonstrates the full workflow:\n",
    "1. Load papers from Zenodo CSV\n",
    "2. Enrich missing abstracts from OpenAlex\n",
    "3. Screen papers against PICO criteria (with checkpoint/resume support)\n",
    "4. Export results\n",
    "\n",
    "Run locally where you have network access to Zenodo/OpenAlex.\n",
    "\n",
    "CHECKPOINT SUPPORT:\n",
    "- Saves results after each paper to checkpoint.jsonl\n",
    "- If interrupted, just run again - already-screened DOIs are skipped\n",
    "- To start fresh: clear_checkpoint(str(CHECKPOINT_FILE))\n",
    "\"\"\"\n",
    "\n",
    "from screeningPaper import (\n",
    "    PICOScreener,\n",
    "    Paper,\n",
    "    load_papers_from_zenodo_csv,\n",
    "    load_papers_from_csv_file,\n",
    "    enrich_paper_from_openalex,\n",
    "    fetch_pdfs_for_papers,\n",
    "    load_results_from_checkpoint,\n",
    "    clear_checkpoint,\n",
    "    checkpoint_summary,\n",
    "    clean_checkpoint,           # NEW: Remove bad entries so they get re-screened\n",
    "    find_suspicious_entries,    # NEW: Find entries that need re-screening\n",
    ")\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "PICO_NANOPUB_URI = \"https://w3id.org/np/RAjO8tdVOla9I77PeXF4iY92ULngrpx5_ZSKFkVrCmsW0\"\n",
    "ZENODO_CSV_URL = None\n",
    "\n",
    "CSV_FILE = \"wildfire-sentinel2-ml/screening_results/included_studies.csv\"\n",
    "\n",
    "\n",
    "PICO_NANOPUB_URI = \"https://w3id.org/np/RAqmVeNbWgL7sNtsr9GqdX0ZTa6aQf3itQmort-JMy4tM\"\n",
    "ZENODO_CSV_URL = None\n",
    "\n",
    "CSV_FILE = \"pets-biodiversity/screening_results/included_studies.csv\"\n",
    "\n",
    "# Project folder - all files go here\n",
    "PROJECT_FOLDER = Path(\"./wildfire-sentinel2-ml\")\n",
    "PDF_FOLDER = PROJECT_FOLDER / \"pdfs\"\n",
    "RESULTS_FOLDER = PROJECT_FOLDER / \"scanning_results\"\n",
    "CHECKPOINT_FILE = RESULTS_FOLDER / \"checkpoint.jsonl\"\n",
    "DEBUG_FOLDER = PROJECT_FOLDER / \"debug\"  # Saves raw LLM responses for uncertain parses\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Load papers from Zenodo CSV\n",
    "# =============================================================================\n",
    "\n",
    "def step1_load_papers(limit: int = None) -> list:\n",
    "    \"\"\"Load papers from Zenodo CSV with abstract enrichment.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Loading papers from Zenodo CSV\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if ZENODO_CSV_URL is not None:\n",
    "        papers = load_papers_from_zenodo_csv(\n",
    "            url=ZENODO_CSV_URL,\n",
    "            enrich_missing_abstracts=True,  # Fetch from OpenAlex if missing\n",
    "            limit=limit  # None for all papers\n",
    "        )\n",
    "    else:    \n",
    "        papers = load_papers_from_csv_file(\n",
    "            filepath=CSV_FILE,\n",
    "            enrich_missing=True,  # Fetch from OpenAlex if missing\n",
    "        )\n",
    "    \n",
    "    # Summary\n",
    "    with_abstract = sum(1 for p in papers if p.abstract)\n",
    "    with_doi = sum(1 for p in papers if p.doi)\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total papers: {len(papers)}\")\n",
    "    print(f\"  With DOI: {with_doi}\")\n",
    "    print(f\"  With abstract: {with_abstract}\")\n",
    "    \n",
    "    return papers\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Pre-download PDFs (optional but recommended)\n",
    "# =============================================================================\n",
    "\n",
    "def step2_download_pdfs(papers: list) -> dict:\n",
    "    \"\"\"Pre-download PDFs from Unpaywall for faster screening.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Downloading PDFs (optional)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    PDF_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stats = fetch_pdfs_for_papers(\n",
    "        papers=papers,\n",
    "        pdf_dir=str(PDF_FOLDER),\n",
    "        skip_existing=True  # Don't re-download\n",
    "    )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Screen papers against PICO (with checkpoint support)\n",
    "# =============================================================================\n",
    "\n",
    "def step3_screen_papers(papers: list) -> list:\n",
    "    \"\"\"Screen papers against PICO research question with checkpoint/resume.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 3: Screening papers against PICO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ensure results folder exists for checkpoint\n",
    "    RESULTS_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check existing progress\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        summary = checkpoint_summary(str(CHECKPOINT_FILE))\n",
    "        print(f\"\\nüìÇ Existing checkpoint found:\")\n",
    "        print(f\"   Already screened: {summary['total']}\")\n",
    "        print(f\"   INCLUDE: {summary['include']}, EXCLUDE: {summary['exclude']}, ERROR: {summary['error']}\")\n",
    "        print(f\"   Remaining: {len(papers) - summary['total']}\")\n",
    "    \n",
    "    # Create screener\n",
    "    screener = PICOScreener.from_nanopub_url(\n",
    "        PICO_NANOPUB_URI,\n",
    "        pdf_folder=str(PDF_FOLDER),\n",
    "        model=\"qwen2.5:14b\",  # Or llama3.1:8b, mistral, etc.\n",
    "        char_limit=25000,\n",
    "        debug_dir=str(DEBUG_FOLDER)  # Saves raw responses for uncertain parses\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{screener.get_pico_summary()}\")\n",
    "    \n",
    "    # Check Ollama\n",
    "    if not screener.is_ollama_available():\n",
    "        print(\"\\n‚ö†Ô∏è  Ollama not available!\")\n",
    "        print(\"   Start with: ollama run qwen2.5:14b\")\n",
    "        return []\n",
    "    \n",
    "    # Screen papers (with checkpoint - will skip already-screened DOIs)\n",
    "    screener.screen_papers(\n",
    "        papers, \n",
    "        verbose=True,\n",
    "        checkpoint_file=str(CHECKPOINT_FILE)\n",
    "    )\n",
    "    \n",
    "    # Load ALL results from checkpoint (includes previous runs)\n",
    "    all_results = load_results_from_checkpoint(str(CHECKPOINT_FILE))\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Export results\n",
    "# =============================================================================\n",
    "\n",
    "def step4_export_results(papers: list, results: list):\n",
    "    \"\"\"Export screening results to various formats.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 4: Exporting results\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    RESULTS_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create lookup for paper metadata by DOI\n",
    "    paper_by_doi = {p.doi: p for p in papers if p.doi}\n",
    "    \n",
    "    # Combine results with paper metadata\n",
    "    combined = []\n",
    "    for result in results:\n",
    "        paper = paper_by_doi.get(result.paper_doi)\n",
    "        combined.append({\n",
    "            \"doi\": result.paper_doi,\n",
    "            \"title\": result.paper_title,\n",
    "            \"year\": paper.year if paper else \"\",\n",
    "            \"decision\": result.decision,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"reason\": result.reason,\n",
    "            \"exclusion_code\": result.exclusion_code,\n",
    "            \"matched_population\": result.matched_population,\n",
    "            \"matched_intervention\": result.matched_intervention,\n",
    "            \"screening_time_ms\": result.screening_time_ms\n",
    "        })\n",
    "    \n",
    "    if not combined:\n",
    "        print(\"No results to export.\")\n",
    "        return\n",
    "    \n",
    "    # Export JSON (full details)\n",
    "    json_path = RESULTS_FOLDER / \"screening_results.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "    print(f\"  Saved: {json_path}\")\n",
    "    \n",
    "    # Export CSV (summary)\n",
    "    csv_path = RESULTS_FOLDER / \"screening_results.csv\"\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=combined[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined)\n",
    "    print(f\"  Saved: {csv_path}\")\n",
    "    \n",
    "    # Export included/excluded lists\n",
    "    included = [c for c in combined if c[\"decision\"] == \"INCLUDE\"]\n",
    "    excluded = [c for c in combined if c[\"decision\"] == \"EXCLUDE\"]\n",
    "    \n",
    "    included_path = RESULTS_FOLDER / \"included_papers.csv\"\n",
    "    with open(included_path, \"w\", newline=\"\") as f:\n",
    "        if included:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"doi\", \"title\", \"year\", \"confidence\", \"reason\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows([{k: c[k] for k in [\"doi\", \"title\", \"year\", \"confidence\", \"reason\"]} for c in included])\n",
    "    print(f\"  Saved: {included_path}\")\n",
    "    \n",
    "    excluded_path = RESULTS_FOLDER / \"excluded_papers.csv\"\n",
    "    with open(excluded_path, \"w\", newline=\"\") as f:\n",
    "        if excluded:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"doi\", \"title\", \"year\", \"exclusion_code\", \"reason\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows([{k: c[k] for k in [\"doi\", \"title\", \"year\", \"exclusion_code\", \"reason\"]} for c in excluded])\n",
    "    print(f\"  Saved: {excluded_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nüìä SCREENING SUMMARY:\")\n",
    "    print(f\"   Total screened: {len(results)}\")\n",
    "    print(f\"   INCLUDE: {len(included)} ({100*len(included)/len(results):.1f}%)\")\n",
    "    print(f\"   EXCLUDE: {len(excluded)} ({100*len(excluded)/len(results):.1f}%)\")\n",
    "    \n",
    "    # Exclusion breakdown\n",
    "    if excluded:\n",
    "        print(f\"\\n   Exclusion reasons:\")\n",
    "        from collections import Counter\n",
    "        codes = Counter(c[\"exclusion_code\"] for c in excluded if c[\"exclusion_code\"])\n",
    "        for code, count in codes.most_common():\n",
    "            print(f\"     {code}: {count}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the full screening pipeline.\"\"\"\n",
    "    \n",
    "    print(\"\\nüî¨ SCIENCE LIVE - Systematic Review Screening Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create project folder\n",
    "    PROJECT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Project folder: {PROJECT_FOLDER.absolute()}\")\n",
    "    print(f\"Checkpoint: {CHECKPOINT_FILE}\")\n",
    "    print(f\"Debug responses: {DEBUG_FOLDER} (for uncertain parses)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CHECKPOINT MANAGEMENT OPTIONS (uncomment as needed)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Option A: Start completely fresh\n",
    "    # clear_checkpoint(str(CHECKPOINT_FILE))\n",
    "    \n",
    "    # Option B: Clean bad entries (empty reasons, suspicious INCLUDEs) \n",
    "    #           so they get re-screened with improved parsing\n",
    "    # clean_checkpoint(str(CHECKPOINT_FILE))\n",
    "    \n",
    "    # Option C: Just see what's suspicious\n",
    "    # suspicious = find_suspicious_entries(str(CHECKPOINT_FILE))\n",
    "    # print(f\"Found {len(suspicious)} suspicious entries:\")\n",
    "    # for s in suspicious[:5]:  # Show first 5\n",
    "    #     print(f\"  - {s['paper_title'][:50]}... ({s['decision']}, conf={s['confidence']})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    \n",
    "    # Step 1: Load papers\n",
    "    papers = step1_load_papers(limit=None)  # Set limit=10 for testing\n",
    "    \n",
    "    if not papers:\n",
    "        print(\"No papers loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Download PDFs (optional - screening will download on-demand)\n",
    "    # Uncomment to pre-download:\n",
    "    # step2_download_pdfs(papers)\n",
    "    \n",
    "    # Step 3: Screen papers (with checkpoint - safe to interrupt and resume)\n",
    "    results = step3_screen_papers(papers)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results. Check Ollama status.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Export\n",
    "    step4_export_results(papers, results)\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline complete!\")\n",
    "    print(f\"   Results in: {RESULTS_FOLDER.absolute()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b161d-097a-4c5e-8a7a-f6d03476b150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
